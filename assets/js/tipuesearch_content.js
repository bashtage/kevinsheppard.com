var tipuesearch = {
  "pages": [
    {
      "title": "Financial Econometrics II: Week 5",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-5/"
    },
    {
      "title": "Financial Econometrics II: Week 6",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-6/"
    },
    {
      "title": "Financial Econometrics II: Week 7",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-7/"
    },
    {
      "title": "Financial Econometrics II: Week 8",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-8/"
    },
    {
      "title": "Installing the MFE package",
      "text": "The mfe package is one I use to prepare the slides.  It is not polished, but I am happy to\nshare it so that you can install it on your computer.  This is not a requirement, but is necessary\nif you want to run the notebook code I have shared.\n\nDownload the MFE package to your computer and make note of the download location.\nInstall the package using pip.  You can do this inside IPython or Jupyter by entering the command\n\n%pip install <full-path-to>/mfe-1.0.tag.gz\n\n\n<full-path-to> should be the path to the gzip. Windows users should usually place mfe-1.0.tag.gz in the same directory\nas the Jupyter notebook so that the package can be installed using \n%pip install mfe-1.0.tag.gz",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/running-notebooks/"
    },
    {
      "title": "Financial Econometrics II: Week 1",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-1/"
    },
    {
      "title": "Financial Econometrics II: Week 2",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-2/"
    },
    {
      "title": "Financial Econometrics II: Week 3",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-3/"
    },
    {
      "title": "Financial Econometrics II: Week 4",
      "text": "Come back in Hilary for updated information",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term-4/"
    },
    {
      "title": "Advanced Financial Econometrics: Forecasting",
      "text": "Reading List\u00b6\nThe reading list contains a set of papers related to the course content. \nGitHub Repository\u00b6\nThe notebooks and data are shared through the course's GitHub repository.\nAssignment\u00b6\nThe AFE group work asks you to use the method presented in the course on a subset of the M4 data.\nLectures\u00b6\nEach of the lectures is presented in small blocks. You can find the presentation\non the YouTube lecture list.\nLecture Slides and Notebooks\u00b6\n\n\n\nSlides\nNotebook\n\n\n\n\nIntroduction\n\n\n\nSARIMA: Seasonal Autoregressive Integrated Moving Average Models\nSARIMA\n\n\nExponential Smoothing\nExponential Smoothing\n\n\nThe Theta Method\nThe Theta Method\n\n\nThe CARD Method\nThe CARD Method\n\n\nForecast Combination Coming Soon\nForecast Combination Coming Soon",
      "tags": "econometrics,mfe",
      "url": "https://www.kevinsheppard.com/teaching/mfe/advanced-financial-econometrics-forecasting/"
    },
    {
      "title": "Scanning Documents for Online Tutorial and Exam Submission",
      "text": "You can also download a pdf version of this recommendation.\n\n\nFlatbed Scanners\nPhone and Tablet Scanning\nGeneral Instructions\nSoftware Recommendations\nAdditional Issues\n\n\n\n\nThis document describes how you can use your phone or tablet to produce clear scans of\nyour work as PDF files that are modest in size. The final section is written for Trinity\nTerm 2020 open-book test-takers since these students may be scanning documents under a\ntight time constraint. If you have any comments of know of software that works better,\nplease let me know at\nkevin.sheppard@economics.ox.ac.uk.\nFlatbed Scanners\u00b6\nIf you have access to a flatbed scanner, this will produce the highest quality scan. I\nrecommend you use 300dpi in black and white mode when scanning documents for submission.\nThis balances file size against the image's resolution. Grayscale and color generally\nproduce files that are larger and that are not usually easier for assessors or tutors to\nread. \nPhone and Tablet Scanning\u00b6\nGeneral Instructions\u00b6\nLighting\u00b6\nPoor lighting is the simplest method to produce a scan that is hard or impossible to\nread. Side lighting is key to creating a high-quality scan when using a phone or tablet.\nOverhead lighting is your enemy since it is likely that your body or your device will\ncast an unnecessary shadow. Look around your environment for locations that have a\nreliable source of side lighting. When scanning in the day, indirect outside light can\nproduce an excellent source for producing high-quality scans. Direct sunlight can lead\nto overexposure and is not recommended. Alternatively, you might find locations with\nunder cabinet lighting. I have good experience using these. Finally, a desk lamp can be\npositioned to produce side lighting where both you and your device will not case a\nshadow. If you do not have a good location with side lighting, you can use your device's\nflash. In my experience, this produces slightly worse quality scans than having a good\nlighting setup, but it is much better than having a shadow on your document. The image below\nshows a scan created with uniform lighting that originated from the side.\n\nTh next scan was created with a strong shadow that was produced by my arm. This tricked the\nsoftware into thinking that this was a color document.\n\nThis final scan was produced by Genius Scan. The device cast a strong shadow on the page\nwhich resulted in a large dark area.\n\nBackground\u00b6\nAssuming you are writing on white paper, you want to scan on a background that has high\ncontrast. Black, dark brown, and most other dark colors produce high contrast, which\nlets the software function better. The next image shows my preferred\nscanning location in my home. \n\nIt is a dark kitchen worktop which provides great\ncontrast. It is illuminated by under cabinet lighting, and so I cannot cast shadows with\nmy body or my device. Note that when scanning, I turn off the other lights in the\nkitchen so that the under-counter lights are the strongest source of illumination.\nDevice Position\u00b6\nIt is important that you hold your device level and that the document occupies the\nmajority of the viewfinder when scanning. Most high-quality scanning software will\nencourage optimal camera positioning.\nMulti-page Documents\u00b6\nMost documents you scan will consist of multiple pages. You should scan these\nconsecutively by adding a new page to your current scan. If you make a mistake, I\nrecommend reshooting the problematic page and then continuing to the end of the\ndocument. All of the recommended software lets you delete a page after you finish\nscanning.\nSoftware Recommendations\u00b6\niOS (iPad/iPhone)\u00b6\nThere are two apps I recommend: Genius\nScan (4.8 rating,\n35,000 reviews) and Scanner\nPro (4.9 rating\n130,000 reviews). Genius Scan is free and has a paid upgrade. It works well enough in\nthe free mode to use for exam and assignment submission. Scanner Pro has a modest price.\nBoth of these apps have great auto edge detection when used on a high contrast\nbackground. ScannerPro is more forgiving to uneven lighting and tends to produce softer\nshadows if present. ScannerPro is my recommendation if the price is not an issue.\nAndroid\u00b6\nGenius\nScan\n(4.7 rating, 78,000 reviews) is also available for Android and is, in my experience, the\nmost accurate scanner.\nCamScanner\n(4.8 rating, 2.5M reviews) also works well, although it is not as accurate at automatic\nedge detection. It does have some party trick modes that work well if you have a text\nthat is mostly monochrome text but contains a full-color diagram. Genius Scan is my\nrecommendation if you have a good lighting setup. If not, I would use CamScanner.\nThere are many other scanners, so please do try others if you are not happy with\nthese. \nGenius Scan Settings\u00b6\n\n\nSet Image Quality to medium before scanning your document (Menu > Settings > Image Quality > Medium).\n  I Could not detect any relevant differences when scanning black and white documents\n  across Medium, High, and Highest.  File sizes, however, do vary with this setting, and Medium\n  minimizes the output size of the PDF without compromising legibility.\n\n\nEdit images before exporting. After you have scanned the document, you can tap on any page to\n  open the edit view.  Editing lets you:\n\n\nAdjust the scanning mode to select Black & White if color incorrectly was detected;\n\nRotate the images so that they have the correct orientation; and\n\nRecrop any images where GeniusScan's edge detection was not adequate.\n\n\nChange the File size when exporting.  GeniusScan will estimate the size of the file after\n  export. In my tests, Small produced files that are easy to read (about 600kB per scanned page,\n  when used with Medium image quality). The table below shows the estimated size of the file\n  exported using combinations of Image Quality and File size.  Medium/Medium and Medium/Large\n  appear to be the most sensible choices. Both Highest Image Quality and Actual File size should\n  be avoided. \n\n\n\n\n\nImage Quality\nSmall\nMedium\nLarge\nActual\n\n\n\n\nLow\n571kB\n640kB\n811kB\n2.1MB\n\n\nMedium\n1.2MB\n1.3MB\n1.7MB\n4.3MB\n\n\nHigh\n2.1MB\n2.3MB\n2.9MB\n7.5MB\n\n\nHighest\n3.6MB\n4.0MB\n5.1MB\n13MB\n\n\n\nI have included two sample files where you can compare\n   Highest/Actual with\n   Medium/Small. Note that the\n   file size of the Medium/Small file is 2.5MB, not 1.2MB, as estimated by GeniusScan. \nExcessively Large Scans\u00b6\nIf you attempt to export and the resulting file is excessively large, you can split a scan\nin GeniusScan. \n\nBegin by selecting the document.\nLong press a page in the second half of the document.\nSelect all pages in the second half.\nSelect the move icon and select New document.\n\nThe animation below illustrates these steps.\n\nYou will then need to rejoin the parts into a single PDF using PDF Sam Basic\nor an online tool.\nExcessively Large PDFs\u00b6\nIf the PDF you have produced exceeds the upload limit, you can compress it using\nan online PDF compressor.  I recommend using the\nmost conservative compression (Less Compression) which reduced scanned file sizes by 50% in \nmy experiments. \nAdditional Issues\u00b6\nI do not believe that at-home exam takers need to be overly concerned with carefully\nformatting their exam answers. If pressed for time, it is reasonable to refer a\nhand-drawn diagram or equation on another page. If you have time, your assessors will\nappreciate your efforts to produce an easier to read answer. If under time pressure and\nyou have two files, combining them into a Zip file before uploading.\nEmbedding Images in Documents\u00b6\nWhen writing essays, you may need one or more diagrams or one or more equations. The best\nformat (as in nicest for your tutor or assessor) is to embed the image into your word document.\nThe simplest method to do this is to:\n\ndraw your diagram;\nphotograph it;\ncrop it tightly on your device; and\nemail it to yourself, upload to a file service (Dropbox, Google Drive, or OneDrive), or let a\n  service like Google Photos or iCloud to automatically back up the photo. You can encourage\n  Google photos to back up your images by opening the Google photos app.  The image will then\n  be available through your browser.\n\nYou should then be able to directly insert the equation or diagram into your document\nwithout further formatting.\nSnipping Tools\u00b6\nSnipping is a simple way to copy anything you can see on your computer into a Word\nor Google docs document.  You can snip just the content you want, and so should not \nneed to crop the image further.\n\nWindows 10: The preferred option is Snip and Sketch. The snipping window can be opened using\n  Win+Shift+S.\nWindows 7/8: These operating systems come with a program called Snipping Tool.\nmacOS: Command + Shift + 4 allows you to select a region to copy.\n\nCombining PDFs\u00b6\nYou may have assignments or exams that require mixed answers where some questions\nare standard essays, while others require more calculation. If your answer is\nspread between a Word document or a Google doc and a scanned PDF, you should combine\nthese into a single PDF before submission.\nThere are two simple methods. The first uses a web service appropriately titled\nCombine PDF. You can upload two or more PDFs, use\ndrag-and-drop to arrange their order, combine them, and then download them. This\nmethods comes with a standard disclaimer about privacy. Combine PDF states they delete\nall content after an hour, but they do not have a Privacy Policy visible.\n\nThe second uses PDF Sam Basic to merge them. PDF Sam is both\nfree to use and open source.  This program has to be locally installed to use. Merging\ntwo or more PDFs can be done by:\n\nselect merge;\nuse Add to add each of your files;\nselect the output pdf file; and\nclick on Run.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/scanning-recommendations/"
    },
    {
      "title": "Financial Econometrics Companion Course",
      "text": "Course Content\u00b6\nThe course covers aspects of financial econometrics including:\n\nSimulation and Monte Carlo\nMaximum Likelihood Estimation\nRegression and Model Selection\nLinear Time Series Models\nVolatility Modeling\nValue-at-Risk Estimation\nVector Autoregressions\n\nThe course follows an outline and is divided into a large number\nof distinct problems.  The problems are grouped into lessons, and\neach is available as a blank Jupyter notebook as well as through a\ncompleted notebook. \nGetting Started\u00b6\nThe simplest method to get the course content is to download the zip of the GitHub repository.\nIf you are comfortable with git, you can clone the repo using \ngit clone https://github.com/bashtage/python-introduction.git\n\n\nThe complete list of lessons appears in the table below.  \n\n\n\nLesson\nDownload\nSolution\n\n\n\n\nDataset Construction\nDownload\nSolution\n\n\nComputing Expectations\nDownload\nSolution\n\n\nSimulation\nDownload\nSolution\n\n\nMethod of Moments\nDownload\nSolution\n\n\nMaximum Likelihood\nDownload\nSolution\n\n\nBias and Variance\nDownload\nSolution\n\n\nRegression\nDownload\nSolution\n\n\nCross-validation in Regressions\nDownload\nSolution\n\n\nRolling and Recursive Regression\nDownload\nSolution\n\n\nBest Subset and Stepwise Regression\nDownload\nSolution\n\n\nRidge Regression and LASSO\nDownload\nSolution\n\n\nTree-based Methods\nDownload\nSolution\n\n\nARMA Model Estimation\nDownload\nSolution\n\n\nARMA Model Forecasting\nDownload\nSolution\n\n\nARMA Model Selection\nDownload\nSolution\n\n\nARMA Model Residual Diagnostics\nDownload\nSolution\n\n\nUnit roots\nDownload\nSolution\n\n\nARCH Model Estimation\nDownload\nSolution\n\n\nARCH Model Selection\nDownload\nSolution\n\n\nARCH Forecasting\nDownload\nSolution\n\n\nValue-at-Risk: Historical Simulation\nDownload\nSolution\n\n\nValue-at-Risk: Filtered Historical Simulation\nDownload\nSolution\n\n\nValue-at-Risk: Forecasting\nDownload\nSolution\n\n\nVector AR Estimation\nDownload\nSolution\n\n\nVector AR Order Selection\nDownload\nSolution\n\n\nGranger Causality\nDownload\nSolution\n\n\nImpulse Response Functions\nDownload\nSolution\n\n\nCointegration Testing using Engle-Granger\nDownload\nSolution",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/python/companion-course/"
    },
    {
      "title": "Financial Econometrics II",
      "text": "Weekly Pages\u00b6\nWeek 1\nWeek 2 \nWeek 3 \nWeek 4 \nWeek 5 \nWeek 6 \nWeek 7 \nWeek 8 \nNotes\u00b6\nSee the notes page for standard and tablet optimized versions of the course notes.\nComputation\u00b6\nSee the Python introduction and the\nPython companion course page for\ncode, data and links to video presentations.\nArchived\u00b6\nThe MFE course has standardized on Python. The MATLAB course page remains\navailable for anyone who wishes to use MATLAB. It has  assignments and data for the MATLAB introduction\nand companion course.",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/hilary-term/"
    },
    {
      "title": "Financial Econometrics I",
      "text": "Cross Sectional Analysis\u00b6\nSlides\u00b6\nPresentation \nPrint Optimized with Notes Area \nPrint Optimized\nWeekly Assignments\u00b6\nWeek 5 Assignment\nNotes\u00b6\nSee the notes page for standard and tablet optimized versions of the course notes.\nComputation\u00b6\nSee the Python introduction and the\nPython companion course page for\ncode, data and links to video presentations.\nArchived\u00b6\nThe MFE course has standardized on Python. The MATLAB course page remains\navailable for anyone who wishes to use MATLAB. It has  assignments and data for the MATLAB introduction\nand companion course.",
      "tags": "mfe,teaching",
      "url": "https://www.kevinsheppard.com/teaching/mfe/michaelmas-term/"
    },
    {
      "title": "Economics and Management Interviews",
      "text": "I am a tutorial fellow at Keble College and interviewed Economics and Management applicants in December 2019 for entrance in 2020. This year we, along with many of my colleagues in other colleges and other subjects in Keble, made a conscious effort to interview more students from economically challenged backgrounds. In the Oxford admissions system, these candidates were classified as Band A and were typically in the bottom 10% of applicants based on a combination of socio-economic factors.\nWhile the interview is only one component of the application process, by the time students arrive at Keble for their interviews, it has substantial weight. The significant influence of the interviews occurs since all candidates have been filtered based on:\n\nGCSE;\ncontextualized GCSE;\nA-level subjects and predicted grades;\nPerformance on the TSA; and\nTheir reference and personal statements.\n\nThe TSA, in particular, is heavily relied upon to reject approximately 80% of candidates.  In general, only candidates in the top 20% were selected for interview. This year we were instructed to give serious consideration to Band A applicants in the top 80% of the TSA score distribution. We were also encouraged to consider Band B candidates. These candidates are outside the bottom 10% based on socio-economic indicators, but still relatively low. There was no specific target score for these applicants to receive an interview.\nAll applicants were interviewed at least twice.  My experiences here only refer to the applicant's economics interview. I found there was a wide range in the apparent level of preparation of the students. The lower socio-economic status students appeared to be meaningfully less prepared. Providing a channel to rectify this gap, as much as possible, is the ultimate motivation for this post.\nThe Interview\u00b6\nCandidates have two 20-minute interviews. The interview is content focused from the beginning. I had asked applicants to read a recent Journal of Economic Perspectives article that discusses some of the complexities of climate change policies. This article is written for a generalist audience, and there are not essential concepts that are covered in A-level Economics required to understand its core messages.\nWe then proceeded to discuss several issues related to the content, as well as some unseen information that I provided as part of the interview. I'm sharing the \"script\" for the interview so that students and teachers can understand the scope of the interview. The script is used to structure the discussion. It is not dogmatically followed since interviews are adapted to the applicant and their background. The supplemental data sheet is also available.\nQuestion 1\u00b6\nThe first question is a warm-up and is used to ensure that the applicant understands the data presented in Figure 1 of the reading. This is a marginal cost curve but is non-standard since the projects are different and many of the projects have negative costs.  The width of the bars demonstrates the project's capacity. The cumulative width of multiple projects is the capacity if all of the projects are implemented.  Some applicants did not fully grasp this final point and thought that the right edge of a project was the project's total capacity (beginning from 0).\nQuestion 2\u00b6\nThis question went deeper and examined whether applicants understood that this is a marginal cost curve.  It is sensible to produce carbon abatement using low-cost technologies first. Many focused on issues other than cost that were not discussed in the article.  It is vital that the material presented is fact-based, and in most cases, that it can be backed up by the facts contained in the paper. It was essential to show mastery of this question before\nproceeding to the next.\nQuestion 3\u00b6\nThis question relies on the  supplemental data sheet that is a simplification of Figure 1 in the\nprereading. \n\nThe total cost curve is a skewed U-shape that begins at zero, declines (but is convex) until 7, is flat from 7 to 8, and then increases from 8.  It crosses the zero around 14. Many applicants did not understand that total costs must start at 0 since you can always do nothing for no cost. Others did not synthesize the material and thought though that marginal cost was the total cost.\nQuestion 4\u00b6\nThis question is a fundamental math question. It assesses whether applicants have mastery of some basic concepts of calculus.  Almost all candidates could not define what a continuous function is (even heuristically, as in \"a function where I do not pick up my pencil\"). Some understood discontinuous function such as $y=tan(x)$, which has many singularities but did not grasp that a function does not need to have an unbounded range to be discontinuous. No one was familiar with continuously differentiable, and so we provided the definition. Most were not confident about how they would verify this. The two-component functions are both clearly continuous and continuously differentiable, and thus only the breakpoint needs to be checked. The question also uses the natural log ($ln$), which is heavily used in economics. Applicants are occasionally insufficiently familiar with the natural logarithm and its inverse, the exponential function.\nQuestion 5\u00b6\nQuestion 5 relies more on the reading and is used to establish a baseline.  Some applicants were not clear as to the fundamental distinction between a static and a dynamic cost: dynamic costs are realized on other projects.  For example, if the UK builds an offshore wind farm, some of the experience in building the wind farm can be used to reduce costs in future offshore wind farms. \nQuestion 6\u00b6\nQuestion 6 asks the applicants to apply their understanding of static and dynamic costs to electric vehicle charging points.  This question is challenging since there are arguably no static cost savings.  The static cost could be reasoned to be infinite if the number of tons of CO2 saved is 0. All, or the vast majority, of the cost, is dynamic since charging points encourage the adoption of electric vehicles.\nQuestion 7\u00b6\nQuestion 7 is open-ended. Applicants may discuss extended issues from the paper, such as the critical role uncertainty plays in dynamic costs or the estimated social cost of emissions.\nPreparing for an interview\u00b6\nInterviews in economics do not follow a single pattern.  Over the past ten years, I have used combinations of:\n\ncomplex logic puzzles that are ultimately applications of game theory;\nmath questions, especially using an approximation in graphical analysis to demonstrate an understanding of a topic\nreadings that can be deconstructed using economic analysis; and\ninterpretation of data (e.g., the trends in inequality over time, which I used in the past).\n\nThere is no single approach to preparing. My (admittedly generic) advice to prepare for an interview in Economics includes:\nThink like an economist - There are several core principals on which economists broadly agree.  A non-exhaustive list includes:\n\nThinking at the margin: what is the cost or benefit of a small change?\nOpportunity Cost: what do we need to give up to choose a particular course of action?\nComparative Advantage: how can two parties aggregate their opportunity costs to improve the outcome for both?\nCorrelation vs. causation: When can data or statistics be interpreted as causal rather than merely capturing a correlation?\n\nThese, and related, principles should be employed when structuring responses.\nBuild on facts - The reading should provide a solid basis for the subsequent discussion.  It is essential to emphasize the material in the passage that supports your point. It is equally crucial to not divert into a tangent that cannot be supported with the text, even if the alternative point may be valid. \nKnow your personal statement - You should be familiar with the details of your personal statement.  This is especially true of specific points you make about how events or readings have influenced your decision to pursue economics.\nPractice Practicing a wide range of problems is the best method to prepare for the uncertainty in an Economics interview. While the interview is not designed or intended to trip you up, the range of possible questions is wide.  \nThe most fruitful strategy is to prepare for as wide a range of problems as possible. This is the same advice I provide to candidates when they ask about the TSA: practice, practice, and then practice. Practice thinking about problems with little notice that do not have definitive answers. Candidates should ensure that they are comfortable communicating arguments verbally. Verbal communication of challenging concepts is not simple and requires practice. Most candidates are adept at producing written work. I have found that not all applicants are equally comfortable verbalizing their logic.  While the interview is not designed to trip up candidates, it is also not intended to be easy.  It is also not a competition in \"fact recollection.\" It is essential that candidates apply economic reasoning - not political, sociological, or philosophical - to the problem. Readings are chosen to provide a fact base that is sufficient to last the entirety of the interview. It is not usually necessary to introduce facts that are not readily verified.\nPerforming Well\u00b6\nThe interview is short and intense.  Candidates need to be ready to demonstrate their subject mastery and understand of the material. Anyone applying to Oxford must be able to communicate their ideas verbally.  Verbal communication is essential in the Oxford education system, where students spend hundreds of hours in small groups settings over their degree. Assessing the student's ability to communicate verbally, as well as to make reasoned arguments, is the justification of the interview.  I have performed interviews over many years. Not all 17-year-old applicants are in a place where they can take full advantage of Oxford's peculiar, labor-intensive, but effective method of teaching. My belief does not reflect on their intellect, their motivation, or their preparation. I do believe that an applicant's performance in an interview indicates whether they are ready to be placed into the unique setting of a tutorial.\nI have no doubts that all of the candidates I have been this year, whether we are in a position to offer them a place or not, will go on to great things.",
      "tags": "economics-and-management,interviews,oxford,teaching",
      "url": "https://www.kevinsheppard.com/blog/economics-and-management-interviews/"
    },
    {
      "title": "403 Forbidden",
      "text": "403 Forbidden\u00b6\nYou are not allowed to see this content.  You are probably here because you \nare visiting a directory, and directory browsing is disabled. This site has\nrecently been reorganized and so you might have ended up here from bad link\nin a search engine.  You can try the searching using the form above or\nnavigate to my landing page.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/403/"
    },
    {
      "title": "50x Internal Error",
      "text": "50x Internal Error\u00b6\nSomething has gone very badly wrong.  This is really unexpected. You can try\nthe searching using the form above or navigate to my landing page. If you\nkeep seeing this page, could you please let me know.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/50x/"
    },
    {
      "title": "404 Not Found",
      "text": "404 Not Found\u00b6\nThe page you are looking for has not been found. This site has recently been reorganized\nand so you might have ended up here from bad link in a search engine.  You can try\nthe searching using the form above or navigate to my landing page.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/404/"
    },
    {
      "title": "Installing Anaconda on Windows",
      "text": "",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/videos/anaconda-widows/"
    },
    {
      "title": "Installing Anaconda on Linux",
      "text": "",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/videos/anaconda-linux/"
    },
    {
      "title": "UCSD Garch",
      "text": "DEPRECATED\nThe UCSD GARCH has been deprecated and will receive no further \nupdates. Recent changes in MATLAB have broken many of the functions in \nthe UCSD GARCH toolbox. Please use the MFE Toolbox which is the \nsuccessor to the UCSD GARCH toolbox.\n\nLegacy UCSD Toolbox\u00b6\n\n\nLegacy UCSD Toolbox\nHelp and Documentation\n\n\nLicense\n\n\nBefore reporting bugs, please be sure you have the\nlatest version, have downloaded the JPL toolbox, and ARE NOT using the\nucsd_garch code from the JPL toolbox (and don't have that directory on\nyour path)\nThe UCSD_Garch toolbox is a toolbox for Matlab that is useful in\nestimating and diagnosing univariate and multivariate heteroskedasticity\nin a Time Series models. The toolbox contains C-Mex files for the\nnecessary loops in the univariate models. It is being released under a\nBSD style [license]. This means you can do pretty much what ever you\nwant to including make money by selling it.\nDownload\nUpdates\u00b6\n2.0.14: Thanks for Mark Flood who pointed out an old bug in fattailed\ngarch. Even more reason to move to the MFE Toolbox if possible.\n2.0.13: Thanks for Mark Flood who pointed out an initialization bug in\nfull_bekk_simulate.\n2.0.12: Thanks to Dennis Turk who pointed out a bug in garchcore.m.\n2.0.11: I have updated the mex files to work with more modern versions\nof MATLAB and removed the 5.3 binaries. I also replaced the missing\ntarchcore.m so that all functions in the toolbox run with or without\nusing the binary files.\n2.0.10: New versions of bsds, bsds_studentized, block_bootstrap and\nstationary_bootstrap that use the latest version of Hansen's SPA paper\n2.0.9.: Quite a few bug/inconsistencies squashed thanks to Paul Koufalas\nand Lance Young\n2.0.8: 2 bugs in dcc_mvgarch and one in egarch squashed\n2.0.7: A few more bugs squashed thanks to Hansen Chen\n2.0.6: Fixed a couple of typos in the skewt garch functions\n2.0.5: Fixed a bug in vech()\n2.0.4: There is a Matlab limitation on filename length of 31 characters\non some versions. dagonal_bekk_mvgarch_likelihood was 1 character too\nlong. It has been renamed diagonalBekkMVgarchLikelihood.\n2.0.4: A bug was found in GARCHINMEAN. It is now fixed.\n2.0.3: A huge bug was found in EGARCH. The original file was using\nvariances, not std devs. This is now fixed. Not> I am unable to build\nthe 5.3 binary as I am out of the country. For now, egarchcore.dll is\nonly available for 6 and above.\nNote: A few last minute bugs have been caught and the toolbox has been\nfixed(Again!). Please fell free to contact ma about any errors you get\nat kevin.sheppard@economics.ox.ac.uk.\nHelp and Documentation\u00b6\nUCSD_GARCH Toolbox, Version 2.0.10 21-APR-2007 \n\nucsd_garch_demo - A demo of the garch toolbox\n\nMain Univariate Mean Functions\u00b6\n\narmaxfilter - Univariate ARMAX estimation\nmafilter - Univariate MA estimation\ngarchinmean - Univariate Garch-In-Mean estimation\n\nMain Univariate GARCH Functions\u00b6\n\ngarchpq_eviews - Univariare GARCH estimation without lower bound constraints; uses a penalty funcion(similar to eviews)\nskewt_garch - Univariat GARCH estimation with skew-t residuals(Hansen)\ntarch - Univariate TARCH and GJR estimation\ngarchpq - Univariate garch estimation with analytic derivatives\nfattailed_garch - Univariate GARCH estimation with normal, Students T and Generalized Error Distribution\nmulti_garch - Univariate GARCH proceeedure to estimate a variety of GARCH specifications including AP GARCH\negarch - Exponential garch estimation with normal, Students T and Generalized Error Distribution\n\nMain Multivariate Functions\u00b6\n\ncc_mvgarch - Estimates Bollerslev's Constant Correlation MV Garch\ndcc_mvgarch - Estimates Engle and Sheppard's Dynamic Correlation MV Garch\no_mvgarch - Estimates Orthogonal or Factor MV Garch\nscalar_bekk_mvgarch - Estimates Engle and Kroner's Scalar Bekk MV Garch\ndiagonal_bekk_mvgarch - Estimates Engle and Kroner's Diagonal Bekk MV Garch\nfull_bekk_mvgarch - Estimates Engle and Kroner's Bekk MV Garch\nIdcc_mvgarch - Estimates Engle and Sheppards Integrated DCC MV Garch\nscalar_bekk_T_mvgarch - Estimates Scalar Bekk MV Garch with Multivariate T disturbances\ndiagonal_bekk_T_mvgarch - Estimates Diagonal Bekk MV Garch with Multivariate T disturbances\nfull_bekk_T_mvgarch - Estimates Full Bekk MV Garch with Multivariate T disturbances\n\nUnivariate Mean and GARCH Simulation\u00b6\n\narmaxsimulate - Simulate an ARMAX model\ngarchsimulate - Sumilate Univariate GARCH series with normal innovations\nfattailed_garchsimulate - Simulate Univariate GARCH series with Normal, Students T, or GED innovations\ngarcheviewssimulate - Simulate a GARCH process with (some)negative smoothing terms\ngarchinmeansimulate - Simulate a garch in mean model\negarchsimulate - Simulate an EGARCH model\nmultigarchSimulate - Simulate one of 8 different forms of GARCH\ndcc_univariate_simulate - likelihood function called from dcc_univariate_simulate\n\nMultivaraite GARCH Simulation\u00b6\n\nscalar_bekk_simulate - Simulate a scalar BEKK\ndiagonal_bekk_simulate - Simulate a diagonal BEKK\nfull_bekk_simulate - Simulate a full BEKK model\ncc_mvgarch_simulate - Simulates Bollerslev's Constant Correlation MV Garch\ndcc_simulate - Simulates Engle and Sheppard's Dynamic Correlation MV Garch\n\nUnivariate Mean Likelihood functions\u00b6\n\ngarchinmeanlikelihood - Likelihood funtion for garch in mean estimation\nmaxfilter_likelihood - Likelihood function for MA estimation\narmaxfilter_likelihood.m - likelihood function called from armaxfilter\n\nUnivariate GARCH Likelihood Functions\u00b6\n\ngarcheviewslikelihood - likelihood function called from garchpq_eviews\nskewt_garchlikelihood - likelihood function called from skewt_garch\nskewtdis_LL - Log likelihod of a skew T distribution(helper)\ngarchlikelihood - likelihood function called from garchpq\nfattailed_garchlikelihood - likelihood function called from fattailed_garch\nmulti_garchlikelihood - likelihood function called from multi_garch\negarchlikelihood - likelihood function called from egarch\n`tarchlikelihood\n\nMultivariate GARCH Likelihood Functions\u00b6\n\ncc_mvgarch_full_likelihood - likelihood function called from cc_mvgarch_full_likelihood\ndcc_mvgarch_full_likelihood - likelihood function called from dcc_mvgarch_full_likelihood(correct)\ndcc_mvgarch_likelihood - likelihood function called from dcc_mvgarch_likelihood(restricted)\ndiagonal_bekk_mvgarch_likelihood - likelihood function called from diagonal_bekk_mvgarch_likelihood\nfull_bekk_mvgarch_likelihood - likelihood function called from full_bekk_mvgarch_likelihood\nscalar_bekk_mvgarch_likelihood - likelihood function called from scalar_bekk_mvgarch_likelihood\nIdcc_mvgarch_full_likelihood - likelihood function called from IDCC_mvgarch_likelihood(correct)\nIdcc_mvgarch_likelihood - likelihood function called from IDCC_mvgarch_likelihood(used in estimation)\nscalar_bekk_T_est_likelihood - likelihood function called from scalar_T_bekk_mvgarch_likelihood(used in estimation)\ndiagonal_bekk_T_est_likelihood - likelihood function called from diagonal_T_bekk_mvgarch_likelihood(used in estimation)\nfull_bekk_T_est_likelihood - likelihood function called from full_T_bekk_mvgarch_likelihood(used in estimation)\nscalar_bekk_T_likelihood - likelihood function called from scalar_T_bekk_mvgarch_likelihood(correct)\ndiagonal_bekk_T_likelihood - likelihood function called from diagonal_T_bekk_mvgarch_likelihood(correct)\nfull_bekk_T_likelihood - likelihood function called from full_T_bekk_mvgarch_likelihood(correct)\n\nDiagnostics\u00b6\n\ndcc_mvgarch_test - Engle and Sheppards test for dynamic correlation\nlilliefors - Lillifors test for normality\nljq2 - Ljung-Box Q Test\nlmtest1 - Lagrange Multiplier Test for autocorrelation\nlmtest2 - Lagrange Multiplier Test for autocorrelation in the squarred residuals, an ARCH test\njarquebera - Jarque-Bera test for normality\nshapirowilks - Shapiro-Wilks Test for normality\nshapirofrancia - Shapiro-Francia Test for normality\nkolmogorov - Kolmorogov-Shmirnov non-parametric test\nberkowitz - The berkowitz transform of the KS test\n\nKernel Smoothing Routines\u00b6\n\ncosinus - Cosinus kernel\nepanechnikov - Epanechnikov kernel\nkern_dens_contour - Bivariate kernel density plot of a density contour\nkern_dens_plot - Univariate kernel density plot\nkern_dens_plot2 - 3d bivariate kernel density plot\nnormal - Normal kernel\nquartic - Quaritc kernel\ntriangular - Triangular kernel\ntriweight - Triweight kernel\nuniform - Uniform kernel\n\nBootstrap Routines\u00b6\n\nblock_bootstrap - Block time series bootstrap\nbsds - Bootstrap Data Snooper(White 2000, Hansen 2001) with upper, lower and consistent pvals\nbsds_studentized - Bootstrap Data Snooper, using studentized bootstraps(Hansen 2001)\ncont_bootstrap - Continuous Bootstrap for unit root data\nstationary_bootstrap - Stationary Bootstrap(Politis and Romano(1994)) for time series\n\nUnivariate Density Functions\u00b6\n\nexppowcdf - Exponential Power Cumulative Density Function\nexppowrnd - Exponential Power Random number generator\nexppowpdf - Exponential Power Random Probability Density Function\ngedcdf - Generalized Error Distribution Cumulative Density Function\ngedinv - Generalized Error Distribution Inverse CDF\ngedpdf - Generalized Error Distribution Probability Density Function\ngedrnd - Generalized Error Distribution Random Number Generator\nskewtdis_cdf - Skew-T Cumulative Density Function\nskewtdis_inv - Skew-T Inverse CDF\nskewtdis_pdf - Skew-T Probability Density Function\nskewtdis_rnd - Skew-T Random Number Generator\nstdtdis_cdf - Standardized T distribution(unit variance for all nu) Cumulative Density Function\nstdtdis_pdf - Standardized T distribution(unit variance for all nu) Probability Density Function\nstdtdis_rnd - Standardized T distribution(unit variance for all nu) Random Number Generator\n\nHelper Functions\u00b6\n\nkscritical - Lookup table for KS critical values\ncc_ivech - Specialized ivech for correlation matrices\nfx.mat - a data set for foreign exchange return used by the demos\nmulti_garch_paramsetup - helper function for multi_garch\nmulti_garch_constraints - helper function for multi_garch\ndcc_hessian - A modified version of HESSIAN for use in with CC_MVGARCH and DCC_MVGARCH\nivech - Creates a square lower triangular matrix, inverse of vech\nvech - Takes teh half-vec of a square matrix, inverse of ivech\nlagmatrix - Returns a matrix of lags of a dependant variable\npca - Performs Principal Componet Analysis\n\nC-MEX functions(should be compilable using any C compiler, binaries for Win32 provided)\nNOTE: WHILE .M FILESARE AVAILABLE FOR ALL OF THESE, YOU SHOULD COMPILE THESE OR USE THE PROVIDED BINARIES\nBINARIES END IN .DLL, MATLAB FUNCTIONS END IN .M, AND SOURCE ENDS IN .C\n\narmaxcore - Core routine for ARMAX\negarchcore - Core routine for EGARCH\ngarchcore - Core routine for GARCH and FATTAILED_GARCH\ngarchgrad - Core routine for GARCH derivative estimation\ngarchinmeancore - Core routine for Garch in mean estimation\nmultigarchcore - Core routine for MULTIGARCH\nivech - C version of ivech\nvech - C version of vech\nmaxcore - Core routing for MA estimation\nrecserarcore - C core for recserar\ntarchcore - Core routine for TARCH estimation\nmultigarchcore - Core routine for MULTIGARCH\n\nNOTE: This toolbox requires both MATLAB optimization toolbox and the excellent J.P.LeSage Library\navailable from www.spatial-econometrics.com\nCopyright (c) 2001-2007 Kevin Sheppard All Rights Reserved.\nLicense\u00b6\nAll of the documentation and software included in the UCSD_Garch toolbox for Matlab is copyrighted by Kevin Sheppard.\nCopyright 2001-2007 Kevin Sheppard\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. All advertising materials mentioning features or use of this software must display the following acknowledgement: This product includes software developed by the Kevin Sheppard. Neither the name of the University of California at San Diego nor Kevin Sheppard may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY KEVIN SHEPPARD ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL KEVIN SHEPPARD OR UCSD OR THE REGENTS OF THE UNIVERSITY OF CALIFORNIA BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nThe views and conclusions contained in the software and documentation are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of the Regents of the University of California or UCSD.\nPlease feel free to contact the author at kevin.k.sheppard with comments, suggestions, or bugfixes.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/code/matlab/ucsd-garch/"
    },
    {
      "title": "IPython Magics",
      "text": "",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/videos/ipython-magics/"
    },
    {
      "title": "Getting Started with Jupyter Notebooks",
      "text": "",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/videos/jupyter-notebooks/"
    },
    {
      "title": "Core IPython",
      "text": "",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/videos/ipython/"
    },
    {
      "title": "Advanced Econometrics",
      "text": "Elective Presentation\nCourse Outline\n\nCollected Slides\u00b6\nThese downloads contain complete slide sets from the course split into two parts.  The first covers material related to data snooping bias and multiple testing in the context of technical trading.  The second covers forecasting methodologies that are suitable with large number of predictors.\n\nTechnical Trading: Fools Gold? Forecast Evaluation with Many Predictions \nForecasting with Many Predictors\n\nSyllabus\u00b6\n\nSyllabus\n\nAssessments\u00b6\n\nAssignment 1\nAssignment 1 Data - Note - You will need the password distributed in the course email to open the Excel file in this zip.\nAssignment 2\nAssignment 2 Data\n\nCourse Material\u00b6\nWeek 1\u00b6\n\n\n\nWeek\nPresentation\nHandout\nMarkup\n\n\n\n\n1\nPresentation\nHandout\nMarkup\n\n\n2\nPresentation\nHandout\nMarkup\n\n\n3\nPresentation\nHandout\nMarkup\n\n\n4\nPresentation\nHandout\nLecture 4 Demonstration\n\n\n5\nPresentation\nHandout\nMarkup\n\n\n6\nPresentation\nHandout\nMarkup\n\n\n7\nPresentation\nHandout\nMarkup\n\n\n8\nPresentation\nHandout\nMarkup\n\n\n\nReadings\u00b6\nWeek 1\u00b6\n\nChernick Ch 2\nChernick Ch 3\nChernick Ch 4 and 5\nLahiri Chs 2 and 7\nPolitis and White\nPatton, Politis and White\nKreiss and Lahiri\n\nWeek 2\u00b6\n\nBrown & Jennings (1989)\nBrock, Lakonishok & LeBaron (1992)\nBlume, Easley & O\u2019Hara (1994)\nSullivan, Timmermann & White (1999)\nLo, Mamaysky & Wang (2000)\nNeely, Rapach, Tu & Zhou (2010)\nHan, Yang & Zhou (2010)\nBajgrowicz & Scaillet (2012)\n\nWeek 3\u00b6\n\nElliott & Timmermann (2008)\nWhite (2000)\nHansen (2005)\n\nWeek 4\u00b6\n\nRomano & Wolf (2005)\nHansen, Lunde & Nason (2010)\nBajgrowicz & Scaillet (2012)\n\nWeek 8\u00b6\n\nReduced Rank Regression Demonstration Code - Shows\nhow to compute reduced rank regression and the spectral version of regularized reduced rank\nregression in simulated data.",
      "tags": "econometrics,mfe",
      "url": "https://www.kevinsheppard.com/teaching/mfe/advanced-econometrics/"
    },
    {
      "title": "Working Papers",
      "text": "Write your page here.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/research/working-papers/"
    },
    {
      "title": "Publications",
      "text": "Write your page here.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/research/publications/"
    },
    {
      "title": "CV - Kevin Sheppard",
      "text": "Write your page here.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/research/cv/"
    },
    {
      "title": "MFE Financial Econometrics Notes",
      "text": "These notes are used in Financial Econometrics I & II in the M.Sc. in Financial Economics at the \nUniversity of Oxford.  Comments and corrections are welcome. \nNotes\u00b6\nTablet Optimized\u00b6\nThese files contain the same content as the print optimized version but are built with a \ntemplate that makes them more friendly for reading on a tablet or iPad.\n\n\n\nChapter\nPrint Optimized\nTablet Optimized\n\n\n\n\nComplete\nFinancial Econometrics\nTBD \n\n\n1\nChapter 1\nTBD \n\n\n2\nChapter 2\nTBD \n\n\n3\nChapter 3\nTBD \n\n\n4\nChapter 4\nTBD \n\n\n5\nChapter 5\nTBD \n\n\n6\nTBD \nTBD \n\n\n7\nChapter 7\nTBD \n\n\n8\nChapter 8\nTBD \n\n\n9\nTBD \nTBD",
      "tags": "mfe",
      "url": "https://www.kevinsheppard.com/teaching/mfe/notes/"
    },
    {
      "title": "MATLAB Introduction",
      "text": "This set of notes is a detailed introduction of using MATLAB and covers virtually all aspects required to implement\nnew models in MATLAB.  It assumes no knowledge of MATLAB and coverall everything required to complete econometric\nand statistical analysis in MATLAB.\nMATLAB Notes for Econometric and Statistical Analysis\nData\u00b6\nSome sections of the notes makes use of additional data files.\nMATLAB Notes for Econometric and Statistical Analysis Data",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/matlab/notes/"
    },
    {
      "title": "MFE Toolbox",
      "text": "The Oxford MFE Toolbox is the follow on to the UCSD_GARCH\ntoolbox. It has been widely used by students here at Oxford, and represents a\nsubstantial improvement in robustness over the original UCSD GARCH code,\nalthough in its current form it only contains univariate routines.\n\n\nCurrent Version\nCode\nDocumentation\nHigh Level List of Functions\n\n\nCurrent Version\u00b6\nThe latest version, including any work in progress, can be downloaded on\nthe GitHub repository for the MFE Toolbox\n(Direct link to zip).\nLast Updated\u00b6\nJune 7, 2013\nUpdate News\u00b6\nMany changes have occurred since the last release. The most notable are:\n\nA major rework of the subsampling in the Realized code\nModern versions of BEKK (Scalar, Diagonal and Full) and RARCH, a\n    recent model by Diaa Noureldin, Neil Sheppard and me.\nDCC, BEKK and HEAVY are all finally available in this toolbox, and\n    so the retirement of the UCSD GARCH toolbox is almost ready.\nOGARCH and GOGARCH have been added.\nRCC, an alternative to DCC, is also available (by Diaa Noureldin,\n    Neil Sheppard and Kevin Sheppard).\n\nThe next developments should include the TODO include:\n\nSARIMA\nClean up of unused files and more coherent naming\n\nCode\u00b6\nOxford MFE Toolbox\nDocumentation\u00b6\nOxford MFE Toolbox Documentation\nHigh Level List of Functions\u00b6\n\nRegression\nARMA Simulation\nARMA Estimation\nHeterogeneous Autoregression\nInformation Criteria\n\n\nARMA Forecasting\nSample autocorrelation and partial autocorrelation\nTheoretical autocorrelation and partial autocorrelation\nTesting for serial correlation\nLjung-BoxQ Statistic\nLM Serial Correlation Test\n\n\nFiltering\nBaxter-King Filtering\nHodrick-Prescott Filtering\n\n\nRegression with Time Series Data\nLong-run Covariance Estimation\nNewey-West covariance estimation\nDen Hann-Levin covariance estimation\n\n\nNonstationary Time Series\nUnit Root Testing\nAugmented Dickey-Fuller testing\nAugmented Dickey-Fuller testing with automated lag selection\n\n\nVector Autoregressions\nGranger Causality Testing: grangercause\nImpulse Response function calculation\n\n\nVolatility Modeling\nARCH/GARCH/AVARCH/TARCH/ZARCH Simulation\nEGARCH Simulation\nAPARCH Simulation\nFIGARCH Simulation\n\n\nGARCH Model Estimation\nARCH/GARCH/GJR-GARCH/TARCH/AVGARCH/ZARCH Estimation\nEGARCH Estimation\nAPARCH Estimation\nAGARCH and NAGARCH estimation\nIGARCH estimation\nFIGARCH estimation\nHEAVY models\n\n\nDensity Estimation\nKernel Density Estimation\n\n\nDistributional Fit Testing\nJarque-Bera Test\nKolmogorov-Smirnov Test\nBerkowitz Test\n\n\nBootstraps\nBlock Bootstrap\nStationary Bootstrap\n\n\nMultiple Hypothesis Tests\nReality Check and Test for Superior Predictive Accuracy\nModel Confidence Set\n\n\nMultaivariate GARCH\nCCC MVGARCH\nScalar Variance Targetting VECH\nMATRIX GARCH\nDCC and ADCC\nOGARCH\nGOGARCH\nRARCH\n\n\nRealized Measures\nRealized Variance\nRealized Covariance\nRealized Kernels\nMultivariate Realized Kernels\nRealized Quantile Variance\nTwo-scale Realized Variance\nMulti-scale Realized Variance\nRealized Range\nQMLE Realized Variance\nMin Realized Variance, Median Realized Variance (MinRV, MedRV)\nIntegrated Quarticity Estimation\n\n\n\nFunctions Missing from Previous UCSD GARCH Toolbox\u00b6\nThe following list of function have not been updated and so if needed,\nyou should continue to use the UCSD GARCH code.\n\nGARCH in mean\nIDCC MVGARCH\nShapirowilks\nShapirofrancia",
      "tags": "",
      "url": "https://www.kevinsheppard.com/code/matlab/mfe-toolbox/"
    },
    {
      "title": "MFE MATLAB",
      "text": "Solutions are posted after the class that covers the assignment has completed.  Solutions are available both as\nMATLAB Live Scripts, which provide an integrated view of code, text and mathematics and generic m-file scripts.\nLive Scripts are only usable in recent versions of MATLAB.\n\n\nMATLAB Notes\nIntroduction\nIntroduction Solutions\n\n\nCompanion Course\nMichaelmas Solutions\nHilary Solutions\nHelper Function\n\n\n\n\nMATLAB Notes\u00b6\nA complete set of notes covering the core aspects of MATLAB used in\neconometric analysis serves as a reference for the companion course.\nIntroduction\u00b6\nMATLAB Introduction Course\nMATLAB Introduction Course Data\nIntroduction Solutions\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nImporting Data into MATLAB\nImporting Data into MATLAB\n\n\nUsing functions\nUsing functions\n\n\nAccessing elements in matrices\nAccessing elements in matrices\n\n\nProgram flow\nProgram flow\n\n\nLogical statements\nLogical statements\n\n\nTables\nTables\n\n\nGraphics\nGraphics\n\n\n\nCompanion Course\u00b6\nMATLAB Companion Course (Complete)\nMichaelmas Solutions\u00b6\nData and Simulation\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nData Set Construction\nData Set Construction\n\n\nSimulation\nSimulation\n\n\nExpectations\nExpectations\n\n\n\nEstimation and Inference\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nMethod of Moments\nMethod of Moments\n\n\nMaximum Likelihood\nMaximum Likelihood\n\n\nBias and Standard Errors\nBias and Standard Errors\n\n\n\nLinear Regression\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nBasic Linear Regression\nBasic Linear Regression\n\n\nRolling Regressions\nRolling Regressions\n\n\nCross-validation and Model Selection\nCross-validation and Model Selection\n\n\nModel Selection and Out-of-Sample R2\nModel Selection and Out-of-Sample R2\n\n\n\nSupport Files\u00b6\n\nSpecific-to-General\nGeneral-to-Specific\nK-fold Cross Validation\nSimulated Data Generation\nInformation Criteria Calculation\n\nHilary Solutions\u00b6\nData\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nData Set Construction\nData Set Construction\n\n\n\nARMA Models\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nARMA Model Estimation\nARMA Model Estimation\n\n\nARMA Model Selection\nARMA Model Selection\n\n\nARMA Diagnostics\nARMA Diagnostics\n\n\nARMA Forecasting\nARMA Forecasting\n\n\nUnit Root Testing\nUnit Root Testing\n\n\n\nARCH Models\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nARCH Model Estimation\nARCH Model Estimation\n\n\nARCH Model Selection\nARCH Model Selection\n\n\nARCH Model Forecasting\nARCH Model Forecasting\n\n\n\nValue-at-Risk\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nValue-at-Risk using Historical Simulation\nValue-at-Risk using Historical Simulation\n\n\nValue-at-Risk using Filtered HS\nValue-at-Risk using Filtered HS\n\n\nValue-at-Risk Evaluation\nValue-at-Risk Evaluation\n\n\n\nVector Autoregressions\u00b6\n\n\n\nMATLAB Live Script (mlx)\nMATLAB Script (m)\n\n\n\n\nVector Autoregression Estimation\nVector Autoregression Estimation\n\n\nVector Autoregression Order Selection\nVector Autoregression Order Selection\n\n\nVector Autoregression Granger Causality\nVector Autoregression Granger Causality\n\n\nVector Autoregression Impulse Responses\nVector Autoregression Impulse Responses\n\n\nVector Autoregression: Engle-Granger Cointegration Testing\nVector Autoregression: Engle-Granger Cointegration Testing\n\n\n\nHelper Function\u00b6\nData Set Construction\u00b6\n\nCompute Month End Prices\n\nExpectations\u00b6\n\nLognormal Quadrature Target\nExpected Utility\n\nMaximum Likelihood\u00b6\n\nStandardized T Log-likelihood (degree of freedom only)\nStandardized T Log-likelihood for unconstrained optimization\nStandardized T Log-likelihood with mean and variance for unconstrained optimization\nProbit Log-likelihood\nOLS Function\n\nData Files (Raw)\u00b6\n\nRaw Data Files (Zipped)\n\nData Files (mat)\u00b6\nHilary\u00b6\n\nS&P 500 (FRED)\nCore CPI (FRED)\nTerm Premium (FRED)\nDefault Premium (FRED)\nGovernment yields and GDF Deflator Data\nCAY cointegration data\n\nMichaelmas\u00b6\nEquity Index Data\u00b6\n\nS&P 500 and FTSE 100 data\n\nKen French Data\u00b6\n\nKen French's Data\n\nFX Rates\u00b6\n\nGBP-USD Exchange Rate\nEUR-USD Exchange Rate\nAUD-USD Exchange Rate\nJPY-USD Exchange Rate",
      "tags": "matlab,mfe",
      "url": "https://www.kevinsheppard.com/teaching/matlab/mfe-matlab/"
    },
    {
      "title": "Presentations (Beamer) in LyX",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/beamer-presentations/"
    },
    {
      "title": "Exporting Completed Documents",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/exporting/"
    },
    {
      "title": "Adding Custom LaTeX in LyX",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/custom-latex/"
    },
    {
      "title": "The Bibliography",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/bibliography/"
    },
    {
      "title": "Figures",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/figures/"
    },
    {
      "title": "Tables",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/tables/"
    },
    {
      "title": "Adding Math",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/math/"
    },
    {
      "title": "List Environments",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/lists/"
    },
    {
      "title": "Basic Text Input",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/basic-input/"
    },
    {
      "title": "Setting up a New Document and Basic Structure",
      "text": "",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/new-document/"
    },
    {
      "title": "Notebook Specimen",
      "text": "Jupyter Notebooks\u00b6The nbsphinx extension allow notebooks to be seemlessly integrated into a Sphinx website.  This page demonstrates how notebooks are rendered.\n\n\n\n\n\n\n\nMathematics\u00b6MathJax can use used to render mathematical equations. Equations can\nbe rendered either in their own line using double dollar signs\n$$ y_{it} = \\alpha_i + \\gamma_t + \\beta x_{it} + \\epsilon_{it} $$or inline using single dollar signs ($\\LaTeX$).\n\n\n\n\n\n\n\nDataFrames\u00b6pandas DataFrames are rendered with useful markup.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'ints': [1, 2, 3], \n                   'floats': [np.pi, np.exp(1), (1+np.sqrt(5))/2],\n                   'strings': ['aardvark', 'bananarama', 'charcuterie' ]})\n\ndf\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[1]:\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      ints\n      floats\n      strings\n    \n  \n  \n    \n      0\n      1\n      3.141593\n      aardvark\n    \n    \n      1\n      2\n      2.718282\n      bananarama\n    \n    \n      2\n      3\n      1.618034\n      charcuterie\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nPlots and Figures\u00b6matplotlib can be used to produce plots in notebooks\nThis example comes from the matplotlib gallery.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfig, ax = plt.subplots(figsize=(12,8))\n\ndata = np.clip(np.random.randn(250, 250), -1, 1)\n\ncax = ax.imshow(data, interpolation='nearest', cmap=cm.coolwarm)\nax.set_title('Gaussian noise with vertical colorbar', fontsize=16)\nplt.tick_params(labelsize=16)\n\n# Add colorbar, make sure to specify tick locations to match desired ticklabels\ncbar = fig.colorbar(cax, ticks=[-1, 0, 1])\ncbar.ax.set_yticklabels(['< -1', '0', '> 1'])  # vertically oriented colorbar\ncbar.ax.tick_params(labelsize=16)",
      "tags": "",
      "url": "https://www.kevinsheppard.com/notebook-specimen/"
    },
    {
      "title": "Final exam",
      "text": "Final Exam\u00b6This self-grading notebook serves as a final exam for the introductory course.\nIf you have grasped the contents of the course, you should be able to complete\nthis exam.\nIt is essential that you answer each cell by assigning the solution to QUESTION_#\nwhere # is the question number.\nWe will start with a warm-up question that is already answered.\nQuestion 0\u00b6Create a 3-element 1-dimensional array containing the values [1,1,1]\nNote: This answer is not assessed.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: The solution is used as a model\nimport numpy as np\n\nQUESTION_0 = np.ones(3) \n\n\n    \n\n\n\n\n\n\n\nQuestion 1\u00b6Construct the correlation matrix\n$$\\left[\\begin{array}{ccc} 1 & 0.2 & 0.5 \\\\ 0.2 & 1 & 0.8 \\\\ 0.5 & 0.8 & 1 \\end{array}\\right]$$as a NumPy array.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 2\u00b6Construct the correlation matrix\n$$\\left[\\begin{array}{ccc} 1 & 0.2 & 0.5 \\\\ 0.2 & 1 & 0.8 \\\\ 0.5 & 0.8 & 1 \\end{array}\\right]$$as a DataFrame with columns and index both equal to ['A', 'B', 'C'].\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 3\u00b6Load the momentum data in the CSV file momentum.csv, set the column date \nas the index, and ensure that date is a DateTimeIndex.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 4\u00b6Construct a DataFrame using the data loaded in the previous question\nthat contains the returns from momentum portfolio 5 in March and April 2016.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 5\u00b6What is the standard deviation of the data:\n$$ 1, 3, 1, 2,9, 4, 5, 6, 10, 4 $$Note Use 1 degree of freedom in the denominator.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 6\u00b6Compute the correlation matrix of momentum portfolios 1, 4, 6, and 10 as a DataFrame\nwhere the index and columns are the portfolio names (e.g., 'mom_01') in the order\nlisted above.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 7\u00b6Compute the percentage of returns of each of the 10 momentum portfolios\nthat are outside of the interval\n$$ [\\hat{\\mu} - \\hat{\\sigma}, \\hat{\\mu} + \\hat{\\sigma}]$$where $\\hat{\\mu}$ is the mean and $\\hat{\\sigma}$ is the standard deviation computed using\n1 dof.  The returned variable must be a Series where the index is the portfolio\nnames ordered from 1 to 10.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 8\u00b6Import the data the data in the sheet question 8 in final-exam.xlsx into\na DataFrame where the index contains the dates and variable name is the column\nname.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 9\u00b6Enter the DataFrame in the table below and save it to HDF with the key 'question9'. The answer to\nthis problem must be the full path to the hdf file. The values in\nindex should be the DataFrame's index.\n\n\nindex\ndata\n\n\n\n\nA\n6.0\n\n\nE\n2.7\n\n\nG\n1.6\n\n\nP\n3.1\n\n\n\nNote: If you want to get the full path to a file saved in the current directory, \nyou can use\nimport os\n\nfile_name = 'my_file_name'\nfull_path = os.path.join(os.getcwd(), file_name)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 10\u00b6Compute the cumulative return on a portfolio the longs mom_10 and shorts mom_01. The\nfirst value should be 1 + mom_10.iloc[0] - mom_01.iloc[0]. The second cumulative\nreturn should be the first return times 1 + mom_10.iloc[1] - mom_01.iloc[1], and\nso on.  The solution must be a Series with the name 'momentum_factor' and index\nequal to the index of the momentum DataFrame.\nNote: The data in the momentum return file is in percentages, i.e., a return of\n4.2% is recorded as 4.2.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 11\u00b6Write a function named QUESTION_11 that take 1 numerical input x and returns:\n\n$exp(x)$ is x is less than 0\n$log(1+x)$ if x is greater than or equal to 0\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 12\u00b6Produce a scatter plot of the momentum returns of portfolios 1 (x-axis) and 10 using only\ndata in 2016.  Set the x limits and y limits to be tight so that the lower bound is the \nsmallest return plotted and the upper bound is the largest return plotted. Use the 'darkgrid'\ntheme from seaborn.  Assign the figure handle to QUESTION_12.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 13\u00b6Compute the excess kurtosis of daily, weekly (using Friday and the end of the week) and monthly \nreturns on the 10 momentum portfolios using the pandas function kurt. The solution must be a\nDataFrame with the portfolio names as the index ordered form 1 to 10 and the sampling frequencies,\n'daily', 'weekly', or 'monthly' as the columns (in order). When computing weekly or monthly returns\nfrom daily data, use the sum of the daily returns.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 14\u00b6Simulate a random walk using 100 normal observations from a\nNumPy RandomState initialized with a seed of 19991231.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 15\u00b6Defining\n\nimport numpy as np\n\ncum_momentum = np.cumprod(1 + momentum / 100)\ncompute the ratio of the high-price to the low price in each month.  The solution\nshould be a DataFrame where the index is the last date in each month and the columns\nare the variables names.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 16\u00b6Simulate 100 observations from the model\n$$ y_i = 0.2 + 1.2 y_{i-1} - 0.2 y_{i-2} + \\epsilon_i$$where $\\epsilon_i$ is a standard normal shock.  Set $y_0=\\epsilon_0$ and\n$y_1=\\epsilon_0 + \\epsilon_1$. The solution should be a 1-d NumPy array with 100 elements. Use\na RandomState with a seed value of 19991231.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 17\u00b6What is the ratio of the largest eigenvalue to the smallest eigenvalue \nof the correlation matrix of the 10 momentum returns?\nNote: This is called the condition number of a matrix and is a measure of\nhow closely correlated the series are. You can compute the eigenvalues from\nthe correlation matrix using np.linalg.eigs.  See the help of this function\nfor more details.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 18\u00b6Write a function that takes a single input 'x' and return the string\n\"The value of x is \" and the value of x. For example, if x is 3.14,\nthen the returned value should be \"The value of x is 3.14\". The function name\nmust be QUESTION_18.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 19\u00b6Compute the percentage of days where all 10 returns are positive and subtract the\npercentage of days where all 10 momentum returns are negative on the same day.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nQuestion 20\u00b6Write the function QUESTION_20 that will take a single input s, which is a string\nand will return a Series that counts the number of times each letter in s appears in s\nwithout regard to case. Do not include spaces.  Ensure the Series returned as its index sorted.\nHints:\n\nHave a look at value_counts for a pandas Series.\nYou can iterate across the letters of a string using\n\n\nsome_string = 'abcdefg'\nfor letter in some_string:\n    do somethign with letter...\n\nstr.lower can be used to get the lower case version of a string\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/final-exam/"
    },
    {
      "title": "Installation",
      "text": "Installing\u00b6Install Anaconda\u00b6\nDownload the Anaconda Python/R Distribution 2019.07 (or later).\nWhen the download is complete, install into your user account. \n\n\n\n\n\n\n\n\nInstall Visual Studio Code and the Python extension\u00b6\nDownload VS Code and install\nInstall the Python extension by clicking on Extensions and searching for \"Python\"\nOpen the mfe-introduction folder created in the previous step\nCreate a file called second.py and enter\n#%%\n\nprint(\"Python may be harder to learn than other languages since\")\nprint(\"there is rarely a single approach to completing a task.\")\n\n\nClick on Run Cell\n\nNote the #%% makes it a magic cell\n\n\n\n\n\n\n\nInstall Pycharm Professional\u00b6\nDownload PyCharm Professional and install using the 30-day trial. You can get a free \ncopy using your academic email address if you want to continue after the first 30 days.\nOpen PyCharm, and create a new project called mfe-introduction\nOpen File > Setting and select Python Interpreter. Select the Anaconda interpreter if it\nis not already selected.\nCreate a new python file called first.py and enter\nprint(\"Python has a steeper curve than MATLAB but more long-run upside\")\n\n\nRight-click on this file, and select \"Run\".",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/installation/"
    },
    {
      "title": "Lesson 1",
      "text": "Getting Started\u00b6This lesson covers:\n\nOpening a terminal window\nLaunching Jupyter notebook\nRunning IPython in a Terminal\nRunning IPython in Jupyter QtConsole\nExecuting a standalone Python file in IPython\nOptional\nJupyter notebooks in VSCode\nJupyter notebooks in PyCharm Professional\n\n\n\n\n\n\n\n\n\n\nOpening an Anaconda Terminal\u00b6An Anaconda terminal allows python to be run directly.  It also allows other\nuseful programs, for example pip, the Python package manager to be used to\ninstall packages that are not available through Anaconda.\nWindows\u00b6Launch Anaconda Prompt from the start menu.\nOSX and Linux\u00b6Open the terminal (instructions depend on your distribution). If you allowed\nconda to initialize, then you should be ready to call Anaconda\"s python and\nsupporting functions.  If not, you should\n\ncd ~/anaconda3/bin\n./conda init\nand then reopen your terminal.\n\n\n\n\n\n\n\nRunning IPython in a Terminal\u00b6\nOpen a terminal.\nRun IPython by entering ipython in the terminal window. You should see a \nwindow like the one below with the iconic In [1] indicating that you\nare at the start of a new IPython session.\n\n\n\n\n\n\n\n\n\nLaunching Jupyter notebook\u00b6\nLaunch Jupyter Notebook from the Start Menu or launcher.\nChange directory to the location where you store your notebooks.\n\n\n\n\n\n\n\n\n\nExecuting a standalone Python file in IPython\u00b6\nOpen a text editor and enter the following lines. Save the file as\nlesson-2.py. Note that Python is white-space sensitive, and so these\nlines should not not indented.\n\nfrom math import exp, log\n\nx = exp(1)\ny = log(x)\n\nprint(f\"exp(1)={x}, log(exp(1))={y}\")\n\n\nRun the code in an IPython session using %run -i lesson-2.py.  Note: you\nshould create the python file in the same directory as the notebook. \n\nIf everything works as expected, you should see\nexp(1)=2.718281828459045, log(exp(1))=1.0\n\n\n\n\n\n\n\n\nJupyter notebooks in VSCode\u00b6Visual Studio Code (or VS Code) is a\nlightweight IDE that supports adding features through extensions.  The\nkey extension for working with notebooks is \nPython extension for Visual Studio Code.\nWith this extension installed, VS code provides native support for \nJupyter notebooks.\n\nInstall VS Code and the Python extension\nOpen the command palette and enter \"create jupyter\" and select the only\navailable item.\n\nSee the screenshot below for an example of the experience of using Jupyter notebooks in \nVS Code.\n\n\n\n\n\n\n\n\nMagic Python in VSCode\u00b6Visual Studio Code supports Magic Python mode in \nstandard Python files that can be executed cell-by-cell.\n\nInstall VS Code and the Python extension\nSelect File, New and then save your file with the extension .py (e.g., file.py).\nThis is a Python file that supports a cell demarcation using #%% for\ncode cells and #%% [markdown] for cells that contain markdown code.\nNote that markdown text must be either:\n\nSurrounded by triple quotes, e.g. \"\"\"markdown text\"\"\" or \"\"\"markdown text\"\"\"; e.g.,\n\"\"\"\n# Cell Heading\n\nLikeness darkness. That give brought creeping. Doesn\"t may. Fruit kind \nmidst seed. Creature, let under created void god to. Them day was Was\ncreature set it from. Fourth. Created don\"t man. Man. Light fourth\nlight given the he image first multiply after deep she\"d great. Morning \nlikeness very have give also fowl third land beast from moving thing\ncreepeth herb creeping won\"t fifth. Us bring was our beast wherein our\nvoid and green he fruit kind upon a given, saying fruit, moveth face \nforth. His you it. Good beginning hath.\n\"\"\"\n\n\nOr commented # (with a single space) at the start of each line,\n# # Cell Heading\n#\n# Likeness darkness. That give brought creeping. Doesn\"t may. Fruit kind \n# midst seed. Creature, let under created void god to. Them day was Was\n# creature set it from. Fourth. Created don\"t man. Man. Light fourth\n# light given the he image first multiply after deep she\"d great. Morning \n# likeness very have give also fowl third land beast from moving thing\n# creepeth herb creeping won\"t fifth. Us bring was our beast wherein our\n# void and green he fruit kind upon a given, saying fruit, moveth face \n# forth. His you it. Good beginning hath.\n\n\n\n\n\nThe cells have a special button above them that allows the contents to be\nexecuted and the result to be displayed in the interactive window. See the \nscreenshot below for an example of the experience of using VS Code. There \nis also an interactive console at the bottom left where commands can be \ndirectly executed.\n\n\n\n\n\n\n\n\nImporting an exiting notebook into Magic Python\u00b6VS Code only understands Magic Python files as notebook-like documents, and so\n.ipynb files must be converted to use. The process of importing is simple:\n\nOpen a Jupyter notebook file\nClick on Import in the popup that appears.\n\n\n\n\n\n\n\n\n\nExporting Magic Python to a Jupyter notebook\u00b6To export a Magic Python file, open the command palette and enter \"import jupyter\". \nSelect the option to import the notebook.\n\n\n\n\n\n\n\n\nJupyter notebooks in PyCharm Professional\u00b6\nPyCharm Professional is my recommended approach if you are going to use Python\nthroughout the course. It provides the best experience and can be acquired for\nfree using the student program.\nPyCharm Professional has deeply integrated Jupyter Notebooks. To create\nan IPython notebook:\n\nOpen PyCharm Profession\nOpen the directory where your notebooks are stored\nRight-click on the root directory and select New > Jupyter Notebook.\nGive your file a meaningful name, and it will open in the main window.\n\n\nPyCharm uses a special syntax where cells look like code and so can be edited\nlike text. This allows PyCharm to use introspection and code completion on the\ncode you have written, a highly useful set of features. PyCharm stores the\nnotebook in a Jupyter notebook file (.ipynb), which means that you can\ntrivially open it in any other Jupyter notebook aware app.  This differs from\nVS code which stores the file as a play Python\nfile (.py) and requires an explicit export to a Jupyter notebook file.\nA code cell is demarcated using #%% and a markdown cell begins with #%% md.\nBelow is a screenshot of this notebook in PyCharm.\n\nMagic Python in PyCharm\u00b6PyCharm supports Magic Python cell execution. To use Magic Python, you need\nto enable Scientific Mode in the View menu. You can then use #%% to\nindicate the start and end of cells. Individual Cells can be executed in\nthe console by pressing CTRL+Enter.\n\nIn PyCharm, right-click on the root directory and select New > Python File. Give\nyour file a meaningful name.\nEnter\n#%%\nprint(\"This is the first cell\")\n\n#%%\nprint(\"This is not executed when the first cell is run\")\n\n\nEnable Scientific Mode in the View menu.\nRun the first cell by placing you mouse in the cell and pressing CTRL+Enter.\nRun the second cell by clicking on the Play button (arrow) that appears in the\ngutter of the editor.\n\n\nNote: Magic Python in PyCharm only supports python code, and so it is\nnot possible to mix Markdown text and Python in the same file.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-1/"
    },
    {
      "title": "Lesson 10",
      "text": "Accessing Elements in DataFrames\u00b6This lesson covers:\n\nAssessing specific elements in Pandas Series and DataFrames \n\nAccessing elements in an array or a DataFrame is a common task. To begin this\nlesson, clear the workspace set up some vectors and a $5\\times5$ array. These\nvectors and matrix will make it easy to determine which elements are selected\nby a command.\nStart by creating 2 DataFrame and 2 Series. Define x=np.arange(24).reshape(5,5) \nwhich is a 5 by 5 array and y=np.arange(5) which is a 5-element 1-d array.\nWe need:\n\nx_df: A default DataFrame containing x\nx_named: A DataFrame containing x with index \"r0\", \"r1\", ..., \"r4\" and\ncolumns \"c0\", \"c1\", ... \"c4\".\ny_s: A default Series containing y\ny_named: A Series containing y that has the index \"r0\", \"r1\", ..., \"r4\"\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a row by name\u00b6Select the 2nd row of x_name using .loc.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a column by name\u00b6Select the 2nd columns of x_name using  both [] and .loc.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a elements of a Series by name\u00b6Select the 2nd element of y_name using both [] and loc.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting rows and columns by name\u00b6Select the 2nd and 4th rows and 1st and 3rd columns of x_name.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: DataFrame selection with default index and column names\u00b6Select the 2nd and 4th rows and 1st and 3rd columns of x_df.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Series selection with the default index\u00b6Select the final element in y_s\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Subseries selection\u00b6Select the subseries of y_named and y_s containing the first, fourth and fifth element.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nLoad the data in momentum.csv.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\nmomentum.head()\n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting data on a single day\u00b6Select returns on February 16, 2016.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting data in a single month\u00b6Select return in March 2016.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting data in a single year\u00b6Select return in 2016.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting data in a date range\u00b6Select returns between May 1, 2016, and June 15, 2016.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Subset time-series\u00b6Select the data for May 2017 for momentum portfolios 1 and 10.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Select using Months\u00b6Using a slice of YYYY-MM, select the returns for momentum portfolio\n5 in the final 6 months of 2016 as Series\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Ensure DataFrame\u00b6Repeat the previous problem but ensure the selection is a DataFrame.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-10/"
    },
    {
      "title": "Lesson 11",
      "text": "Accessing Elements in NumPy Arrays\u00b6This lesson covers:\n\nAccessing specific elements in NumPy arrays\n\nAccessing elements in an array or a DataFrame is a common task. To begin this lesson, clear the\nworkspace set up some vectors and a $5\\times5$ array. These vectors and matrix will make it easy\nto determine which elements are selected by a command.\nUsing arange and reshape to create 3 arrays:\n\n5-by-5 array x containing the values 0,1,...,24 \n5-element, 1-dimensional array y containing 0,1,...,4\n5-by-1 array z containing 0,1,...,4\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nZero-based indexing\u00b6Python indexing is 0 based so that the first element has position 0, the second has position 1\nand so on until the last element has position n-1 in an array that contains n elements in\ntotal.\nProblem: Scalar selection\u00b6Select the number 2 in all three, x, y, and z.\nQuestion:  Which index is rows and which index is columns?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Scalar selection of a single row\u00b6Select the 2nd row in x and z using a single integer value.\nQuestion: What is the dimension of x and the second row of x\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Slice selection of a single row\u00b6Use a slice to select the 2nd row of x and the 2nd element of y and z.\nQuestion: What are the dimension selections?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: List selection of a single row\u00b6Use a list to select the 2nd row of x and the 2nd element of y and z.\nQuestion: What are the dimension selections?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a single Column\u00b6Select the 2nd column of x using a scalar integer, a slice and a list.\nQuestion: What the the dimensions of the selected elements?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a block of specific columns\u00b6Select the 2nd and 3rd columns of x using a slice.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a block of specific rows\u00b6Select the 2nd and 4th rows of x using both a slice and a list.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting a block of specific rows and columns\u00b6Combine these be combined to select the 2nd and 3rd columns and 2nd and 4th rows.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Use ix_ to select rows and columns using lists\u00b6Use ix_ to select the 2nd and 4th rows and 1st and 3rd columns of x.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Convert a DataFrame to a NumPy array\u00b6Use  .to_numpy to convert a DataFrame to a NumPy array.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Create a DataFrame\nimport pandas as pd\nimport numpy as np\n\nnames = [\"a\", \"b\", \"c\", \"d\", \"e\"]\nx = np.arange(25).reshape((5,5))\nx_df = pd.DataFrame(x, index=names, columns=names)\nprint(x_df)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Use np.asarray to convert to an array\u00b6Use  np.asarray to convert a DataFrame to a NumPy array.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Block selection\u00b6Select the second and third rows of a and the first and last column.\nUse at least three different methods including all slices, np.ix_, and\nmixed slice-list selection.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Data for Exercises\n\nimport numpy as np\nrs = np.random.RandomState(20000214)\na = rs.randint(1, 10, (4,3))\nb = rs.randint(1, 10, (6,4))\n\nprint(f\"a = \\n {a}\")\nprint()\nprint(f\"b = \\n {b}\")\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Row Assign\u00b6Assign the first three elements of the first row of b to a.\nNote Assignment sets one selected block in one array equal to another \nblock.\nx[0:2,0:3] = y[1:3,1:4]\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Block Assign\u00b6Assign the block consisting the first and third columns and the second and last rows of b\nto the last two rows and last two columns of a\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-11/"
    },
    {
      "title": "Lesson 12",
      "text": "Numeric Indexing of DataFrames\u00b6This lesson covers:\n\nAccessing specific elements in DataFrames using numeric indices\n\nAccessing elements in a DataFrame is a common task. To begin this lesson,\nclear the workspace set up some vectors and a $5\\times5$ array. These vectors\nand matrix will make it easy to determine which elements are selected by a\ncommand.\nBegin by creating:\n\nA 5-by-5 DataFrame x_df containing np.arange(25).reshape((5,5)).\nA 5-element Series y_s containing np.arange(5).\nA 5-by-5 DataFrame x_named that is x_df with columns \"c0\", \"c1\", ...,\n\"c4\" and rows \"r0\", \"r1\", ..., \"r4\".\nA 5-element Series y_named with index \"r0\", \"r1\", ..., \"r4\". \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Picking an Element out of a DataFrame\u00b6Using double index notation, select the (0,2) and the (2,0) element of\nx_named.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Select Elements from Series\u00b6Select the 2nd element of y_named.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Rows as Series\u00b6Select the 2nd row of x_named using the colon (:) operator.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Rows as DataFrames\u00b6\nSelect the 2nd row of x_named using a slice so that the selection\nremains a DataFrame.\nRepeat using a list of indices to retain the DataFrame. \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Entire Columns as Series\u00b6Select the 2nd column of x_named using the colon (:) operator.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Single Columns as DataFrames\u00b6Select the 2nd column of x_named  so that the selection remains a DataFrame.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Specific Columns\u00b6Select the 2nd and 3rd columns of x_named using a slice.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Select Specific Rows\u00b6Select the 2nd and 4th rows of x_named using a slice.  Repeat the \nselection using a list of integers.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Select arbitrary rows and columns\u00b6Combine the previous selections to the 2nd and 3rd columns and the 2nd and 4th rows\nof x_named.\nNote: This is the only important difference with NumPy.  Arbitrary\nrow/column selection using DataFrame.iloc is simpler but less flexible.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Mixed selection\u00b6Select the columns c1 and c2 and the 1st, 3rd and 5th row.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Mixed selection 2\u00b6Select the rows r1 and r2 and the 1st, 3rd and final column.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Select fixed length block\u00b6Compute the mean return of the momentum data in the first 66\nobservations and the last 66 observations.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\nmomentum.head()\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Compute values using fraction of sample\u00b6Compute the correlation of momentum portfolio 1, 5, and 10 in the first half of\nthe sample and in the second half.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-12/"
    },
    {
      "title": "Lesson 13",
      "text": "for Loops\u00b6This lesson covers:\n\nfor loops \nNested loops \n\n\n\n\n\n\n\n\nProblem: Basic For Loops\u00b6Construct a for loop to sum the numbers between 1 and N for any N. A for loop\nthat does nothing can be written:\nn = 10\nfor i in range(n):\n    pass\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Compute a compound return\u00b6The compound return on a bond that pays interest annually at rate r is given\nby $cr_{t}=\\prod_{i=1}{T}(1+r)=(1+r){T}$. Use a for loop compute the total\nreturn for \u00a3100 invested today for $1,2,\\ldots,10$ years. Store this variable\nin a 10 by 1 vector cr.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Simulate a random walk\u00b6(Pseudo) Normal random variables can be simulated using the command\nnp.random.standard_normal(shape) where shape is a tuple (or a scalar)\ncontaining the dimensions of the desired random numbers. Simulate 100 normals\nin a 100 by 1 vector and name the result e. Initialize a vector p\ncontaining zeros using the function zeros. Add the 1st element of e to the\nfirst element of p. Use a for loop to simulate a process\n$y_{i}=y_{i-1}+e_{i}$. When finished plot the results using\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(16,6))\n\nplt.plot(y)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Nested Loops\u00b6Begin by loading momentum data used in an earlier lesson. Compute a\n22-day moving-window standard deviation for each of the columns. Store\nthe value at the end of the window.\nWhen finished, make sure that std_dev is a DataFrame and \nplot the annualized percentage standard deviations using:\nann_std_dev = 100 * np.sqrt(252) * std_dev\nann_std_dev.plot()\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport pandas as pd\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\nmomentum = momentum / 100  # Convert to numeric values from percentages\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise\u00b6\nSimulate a 1000 by 10 matrix consisting of 10 standard random walks using\nboth nested loops and np.cumsum. \nPlot the results. \n\nQuestion to think about\nIf you rerun the code in this Exercise, do the results change? Why?\nExercise: Compute Drawdowns\u00b6Using the momentum data, compute the maximum drawdown over all\n22-day consecutive periods defined as the smallest cumulative \nproduce of the gross return (1+r) for 1, 2, .., 22 days.\nFinally, compute the mean drawdown for each of the portfolios.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-13/"
    },
    {
      "title": "Lesson 14",
      "text": "Logical Operators\u00b6This lesson covers:\n\nBasic logical operators \nCompound operators \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Reproducible random numbers\n\nimport numpy as np\nrs = np.random.RandomState(20000101)\n\n\n    \n\n\n\n\n\n\n\nProblem: Basic Logical Statements\u00b6Create the variables (in order)\n\nx as rs.random_sample(), a uniform on $[0, 1)$\ny as rs.standard_normal(), a standard normal ($N(0,1)$)\nz as rs.randint(1, 11), a uniform random integer on $[1, 2,\\ldots, 10]$\n\nCheck whether each of these are above their expected value.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Using comparison operators\u00b6\nCheck if z is 7\nCheck is z is not 5\nCheck if z is greater than or equal to 9\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Combining booleans\u00b6\nDetermine if $2\\leq z < 8$\nDetermine if $z < 2 \\cup z \\geq 8$ using or\nRewrite 2 using not and your result from 1.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Data for Exercise\nimport numpy as np\nrs = np.random.RandomState(19991213)\n\n# Like range, lower included, upper excluded\n# u in (0, 1, 2, ..., 5)\nu = rs.randint(0, 6) \n# v in (-2, -1, 0, 1, 2)\nv = rs.randint(-2, 3) \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise\u00b6Is the product $uv$ 0 and only one of $u$ and $v$ is 0?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise\u00b6Write three logical statements that will determine if $0\\leq u \\leq 2$ and $0\\leq v \\leq 2$.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-14/"
    },
    {
      "title": "Lesson 15",
      "text": "Boolean Arrays\u00b6This lesson covers:\n\nCreating Boolean arrays\nCombining Boolean arrays\nall and any\n\nBegin by loading the data in momentum.csv.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport numpy as np\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\n\nprint(momentum.head())\n\nmom_01 = momentum[\"mom_01\"]\nmom_10 = momentum[\"mom_10\"]\nmom_05 = momentum[\"mom_05\"]\n\n\n    \n\n\n\n\n\n\n\nProblem: Boolean arrays\u00b6For portfolios 1 and 10, determine whether each return is $<0$ (separately).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Combining boolean arrays\u00b6Count the number of times that the returns in both portfolio 1 and portfolio\n10 are negative. Next count the number of times that the returns in portfolios\n1 and 10 are both greater, in absolute value, that 2 times their respective\nstandard deviations.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Combining boolean arrays\u00b6For portfolios 1 and 10, count the number of times either of the returns is $<0$.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Count the frequency of negative returns\u00b6What percent of returns are negative for each of the 10 momentum portfolios?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Use any to find large losses\u00b6Use any to determine if any of the 10 portfolios experienced a loss\ngreater than -5%.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nUse all and negation to do the same check as any.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: all and any\u00b6Use all and sum to count the number of days where all of the portfolio returns\nwere negative. Use any to compute the number of days with at least 1 negative\nreturn and with no negative returns (Hint: use negation (~ or logical_not)).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Count Extreme Days\u00b6Count the number of days where each of the portfolio returns is less than the \n5% quantile for the portfolio. Also report the fraction of days where all are in their\nlower 5% tail.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-15/"
    },
    {
      "title": "Lesson 16",
      "text": "Boolean Selection\u00b6This lesson covers:\n\nBoolean selection\nwhere\n\nBegin by loading the data in momentum.csv.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport numpy as np\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\n\nprint(momentum.head())\n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting rows with boolean conditions\u00b6Select the rows in momentum where all returns on a day are negative.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting rows\u00b6Select the rows in momentum where 50% or more of the returns on a day are negative.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting columns\u00b6Select the columns in momentum what have the smallest and second smallest average returns.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting rows and columns\u00b6Select the returns for the column with the single most negative return\non days where all of the returns are negative.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Selecting Elements using Logical Statements\u00b6For portfolio 1 and portfolio 10 compute the correlation when both \nreturns are negative and when both are positive.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Reproducible random numbers\n\nrs = np.random.RandomState(19991231)\nx = rs.randint(1, 11, size=(10,3))\nx\n\n\n    \n\n\n\n\n\n\n\nProblem: Select the columns of x that means >= $E[x]$\u00b6\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Select the rows of x that means >= $E[x]$\u00b6\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Select the rows and column of x where both have means < $E[x]$\u00b6\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Using where\u00b6Use where to select the index of the elements in portfolio 5 that are\nnegative. Next, use the where command in its two output form to determine\nwhich elements of the portfolio return matrix are less than -2%.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Select the Most Volatile Portfolio\u00b6Select the column in momentum that has the highest standard deviation.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Select the High Kurtosis Portfolios\u00b6Select the columns that have kurtoses above the median kurtosis.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Select\u00b6Select the rows where all of the returns in the row are less than the 25%\nquantile for their portfolio.\nNote: Comparisons between DataFrames and Series works like mathematical\noperations (+, -, etc.).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-16/"
    },
    {
      "title": "Lesson 17",
      "text": "Conditional Statements\u00b6\nif-elif-else blocks\n\n\n\n\n\n\n\n\nProblem: Print value if negative\u00b6Draw a standard normal value using np.random.standard_normal and print the\nvalue if it is negative.\nNote: Rerun the cell a few time to see different output.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Print different messages based on value\u00b6Draw a standard normal value and print \"Positive\" if it is positive\nand \"Negative\" if not.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem:\u00b6Draw a standard t random variable with 2 degrees of freedom using\nnp.random.standard_t(2) and print \"Negative Outlier\" if less than -2,\n\"Positive Outlier\" if larger than 2, and \"Inlier\" if between -2 and 2.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Classify two points\u00b6Generate two standard normal values x and y using\ntwo calls to rs.standard_normal(). Use an if-elif-else \nclause to print the quadrant they are in.  The four quadrants are\nupper right, upper left, lower left and lower right.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Generate a contaminated normal\u00b6Generate a uniform using u = rs.sample(). Using this value and an \nif-else clause, generate a contaminated normal which is a draw from a\n$N(0,1)$ ($N(\\mu,sigma2)$) if $u<0.95$ or a draw from a $N(0,10)$ otherwise.\nUse rs.normal to generate the normal variable.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-17/"
    },
    {
      "title": "Lesson 18",
      "text": "Logic and Loops\u00b6This lesson covers:\n\nMixing logic and loops \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\n\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\n\nmom_01 = momentum.mom_01\nprint(momentum.head())\n\n\n    \n\n\n\n\n\n\n\nProblem: Logical Statements and for Loops\u00b6Use a for loop along with an if statement to simulate an asymmetric random\nwalk of the form\n$$y_{i}=y_{i-1}+e_{i}+I_{[e_{i}<0]}e_{i}$$where $I_{[e_{i}<0]}$ is known as an indicator variable that takes the value\n1 if the statement in brackets is true. Plot y. $e$ is a standard normal\nshock. Use cumsum to simulate a symmetric one (z), and plot the two using\nthe code in the cell below.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nPlot the two random walks using the code.  We will cover data visualization\nin a later lesson.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(y)\nplt.plot(z)\nplt.legend([\"y\", \"z\"])\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Simulate the asymmetric random walk without an if-then\u00b6Use boolean multiplication to simulate the same random walk without using\nan if-then statement.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Plot the data\n%matplotlib inline\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Combining flow control\u00b6For momentum portfolios 1 and 10, compute the length of the runs in the\nseries. In pseudo code,\n\nStart at i=1 and define run(1) = 1\nFor i in 2,...,T, define run(i) = run(i-1) + 1 if \n$\\textrm{sgn}\\left(r_{i}\\right)=\\textrm{sgn}\\left(r_{i-1}\\right)$ else 1.\n\nYou will need to use len and zeros.\n\nCompute the length longest run in the series and the index of the\nlocation of the longest run. Was it positive or negative?\nHow many distinct runs lasted 5 or more days?\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nPlot the runs using\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.plot(run)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Simulate a Process with Heteroskedasticity\u00b6Simulate 100 observations of a time series with heteroskedasticity \nthat follows a random walk of the form:\n$$ y_t = y_{t-1} + \\sigma_t \\epsilon_t $$where $\\epsilon_t\\sim N(0,1)$, $y_0=0$ and $\\sigma_t$ is:\n\n0.5 if the 0 of the past 3 shocks are negative\n1 if 1 of the past 3 shocks are negative\n2 if 2 of the past 3 shocks are negative\n6 if 3 of the past 3 shocks are negative\n\nPlot the result.\nNotes\n\nWhen generating the first 3 values, treat $\\epsilon_{-1}$, $\\epsilon_{-2}$ and\n$\\epsilon_{-3}$ as 0 (non-negative).\nRe-run the simulation to see different paths.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-18/"
    },
    {
      "title": "Lesson 19",
      "text": "Importing Data\u00b6This lesson covers:\n\nImporting data \nConverting dates \n\n\n\n\n\n\n\n\nProblem: Reading in data with Dates\u00b6Read in the files GS10.csv and GS10.xls which have both been downloaded\nfrom FRED.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Converting Dates\u00b6\nLoad the CSV file without converting the dates in read_csv.\nConvert the date column, remove it from the DataFrame, and set it as the\nindex. \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Selectively Load Columns\u00b6\nLoad the data in data/fred-md.csv in the columns sasdate,\nRPI and INDPRO using the usecols keyword.\nRemove the first row by selecting the second to the end.\nConvert sasdate to dates\nSet sasdate as the index and remove it from the DataFrame.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Load and Merge multiple Sheets\u00b6\nLoad the data on the sheet \"Long Mat\" in the Excel file \"data/exercise.xlsx\". \nThese are 10 and 20 year constant maturity yields.\nLoad the data on the sheet \"Short Mat\" in the Excel file \"data/exercise.xlsx\".\nThese are 1 and 3 year constant maturity yields.\nCombine the columns in the two DataFrames by creating a dictionary of the keys in\neach with the values equal to the column names.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-19/"
    },
    {
      "title": "Lesson 2",
      "text": "Basic Python Types\u00b6This lesson covers:\n\nInputting scalars and strings\nLists\nTuples\nDictionaries\n\n\n\n\n\n\n\n\nProblem: Input scalar floating point and integers\u00b6\nCreate a variable called scalar_float containing $\\pi$ to 4 digits.\nCreate a variable called scalar_int containing 31415. \nPrint each value using the print function.   \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a string and an f-string\u00b6\nCreate a variable called a_string containing This is a string\nCreate a f-string the prints The value of scalar_float is 3.1415 using\nthe variable created in the previous step\nCreate two string, first containing String concatenation and the\nsecond containing is like addition, and join the two using + to produce\nString concatenation is like addition. \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a list\u00b6\nCreate a list containing scalar_float and scalar_int\nAdd a_string to the list.\nSelect the second element from the list \nSelect the the lst two elements of the list\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a list of lists\u00b6\nCreate a list containing the two lists [1, 2, 3] and [4, 5, 6]\nSelect the element 5 from the nested list\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a tuple\u00b6\nCreate a tuple containing the values (1, 2.0, \"c\")\nSelect the element \"c\" from the tuple\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Convert a list to a tuple and back\u00b6\nConvert the list-of-lists created to a tuple-of-tuples\nConvert the tuple-of-tuples back to a list of lists\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a dictionary\u00b6\nCreate a dictionary containing the key-value pairs \"float\" and 3.1415,\n\"int\" and 31415, and \"string\" and \"three-point-one-four-one-five\".\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Lookup and Change a value\u00b6\nLook up the value of \"float\".\nChange the value of \"float\" to 22 / 7.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Add and remove a key\u00b6\nAdd the new key \"better_float\" with the value 3.141592.\nRemove the key \"float\" and its value.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Manipulating lists\u00b6\nCreate an empty list called lst\nAdd the elements 9, \"Eight\" and 7.0 (in order) to the list.\nExtend the list with the list [\"Six\", 5, 4.0] using extend\nSelect first 4 elements of lst\nSelect last 3 elements of lst\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Dictionary Manipulation\u00b6\nCreate a empty dictionary called dct\nAdd the pairs \"apple\" and 1, \"banana\" and 2.0, and \"cherry\" and \"iii\"\nReplace the value of \"apple\" with \"I\"\nRemove the entry for \"banana\"\nAdd an entry \"date\" with the value 4\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Directly create a Dictionary\u00b6Using the final verion of dct from the previous exercise:\n\nDirectly initialize a new dictionary called other_dct.\nUse an f-string to print the values associated with each key.\n\nHint You must use both types of quotes. For example, to access a value in\nan f-string.\nf\"{other_dct['apple']}\"\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Tuple Manipulation\u00b6\nCreate a tuple tpl containing 100 and 4\nConvert to a list, add the elements 101 and 5\nConvert back toa tuple\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-2/"
    },
    {
      "title": "Lesson 20",
      "text": "Saving and Exporting Data\u00b6This lesson covers:\n\nSaving and reloading data\n\nThis first block loads the data that was used in the previous lesson.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the data to use later\nimport pandas as pd\n\ngs10_csv = pd.read_csv(\"data/GS10.csv\", index_col=\"DATE\", parse_dates=True)\ngs10_excel = pd.read_excel(\"data/GS10.xls\", skiprows=10,\n                           index_col=\"observation_date\")\n\n\n    \n\n\n\n\n\n\n\nProblem: Export to Excel\u00b6Export gs10_csv to the Excel file gs10-exported.xlsx.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Export to CSV\u00b6Export gs10_excel to CSV.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Export to HDF\u00b6Export both to a single HDF file (the closest thing to a \"native\" format in pandas).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Import from HDF\u00b6Import the data saved as HDF and verify it is the same as the original data.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Import, export and verify\u00b6\nImport the data in \"data/fred-md.csv\"\nParse the dates and set the index column to \"sasdate\"\nRemove first row labeled \"Transform:\" (Hint: Transpose, del and\ntranspose back, or use drop)\nRe-parse the dates on the index\nRemove columns that have more than 10% missing values\nSave to \"data/fred-md.h5\" as HDF.\nLoad the data into the variable reloaded and verify it is identical.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Looping Export\u00b6Export the columns RPI, INDPRO, and HWI from the FRED-MD data to\n\"data/variablename.csv\" so that, e.g., RPI is exported to data/RPI.csv:\nNote You need to complete the previous exercise first (or at least the first 4 steps).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-20/"
    },
    {
      "title": "Lesson 21",
      "text": "Graphics: Line Plots\u00b6This lesson covers:\n\nBasic plotting \nSubplots \nHistograms \nScatter Plots\n\n\n\n\n\n\n\n\nPlotting in notebooks requires using a magic command, which starts with\n%, to initialize the plotting backend.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup\n%matplotlib inline\n\n\n    \n\n\n\n\n\n\n\nBegin by loading the data in hf.h5. This data set contains high-frequency\nprice data for IBM and MSFT on a single day stored as two Series. IBM is\nstored as \"IBM\" in the HDF file, and MSFT is stored as \"MSFT.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Basic Plotting\u00b6\nPlot the ibm series which contains the price of IBM. \nAdd a title and label the axes. \nAdd markers and remove the line. \n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Subplot\u00b6Create a 2 by 1 subplot with the price of IBM in the top subplot and the\nprice of MSFT in the bottom subplot.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Plot with Dates\u00b6Use matplotlib to directly plot ibm against its index. This is a\nrepeat of a previous plot but shows how to use the plot command directly.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Change seaborn\u00b6Produce a line plot of MSFT's price using seaborn's \"whitegrid\" style.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: HLOC plot\u00b6Use the HLOC data to produce a plot of MSFT's 5 minute HLOC\nwhere the there are no lines, high is demarcated using a green triangle,\nlow is demarcated using a red downward pointing triangle, open is demarcated \nusing a light grey leftward facing triangle and close is demarcated using\na right facing triangle.\nNote Get the axes from the first, plot, and reuse this when plotting \nthe other series.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load data and create values\nimport pandas as pd\n\nmsft = pd.read_hdf(\"data/hf.h5\", \"MSFT\")\nmsft_5min = msft.resample(\"300S\")\nhigh = msft_5min.max()\nlow = msft_5min.min()\nopen = msft_5min.first()\nclose = msft_5min.last()\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-21/"
    },
    {
      "title": "Lesson 22",
      "text": "Graphics: Other Plots\u00b6This lesson covers:\n\nHistograms \nScatter Plots\n\n\n\n\n\n\n\n\nPlotting in notebooks requires using a magic command, which starts with %,\nto initialize the plotting backend.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup\n%matplotlib inline\n\n\n    \n\n\n\n\n\n\n\nBegin by loading the data in hf.h5. This data set contains high-frequency\nprice data for IBM and MSFT on a single day stored as two Series. IBM is\nstored as \"IBM\" in the HDF file, and MSFT is stored as \"MSFT.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Histogram\u00b6Produce a histogram of MSFT 1-minute returns (Hint: you have to produce\nthe 1-minute Microsoft returns first using resample and pct_change).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Scatter Plot\u00b6Scatter the 5-minute MSFT returns against the 5-minute IBM returns.\nHint: You will need to create both 5-minute return series, merge them,\nand then plot using the combined DataFrame.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Saving plots\u00b6Save the previous plot to PNG and PDF.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Visualize 5 and 10 minute returns\u00b6Produce a 2 by 1 subplot with a histogram of the 5-minute returns of IBM in the\ntop panel and 10-minute returns of IBM in the bottom. Set an appropriate title on\neach of the 2 plots.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Export the result of the previous exercise to JPEG and PDF\u00b6\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Plot histograms and a scatter plot\u00b6Produce a 2 by 2 subplot with:\n\nCreate a square figure with a size of 10 by 10 using plt.rc\nHistograms of IBM and MSFT on the diagonals\nScatter plots on the off-diagonals where the x and y line up with the\nhistogram on the diagonal.\nSet the limits of the scatter plots to match the appropriate histogram\nx and y limit.\nClean up the plot using tight_layout\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Use pandas plotting tools\u00b6Use pandas.plotting.scatter_matrix to produce a similar plot to the previous exercise.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-22/"
    },
    {
      "title": "Lesson 3",
      "text": "Importing Modules\u00b6This lesson covers:\n\nModule import\n\n\n\n\n\n\n\n\nProblem: Importing Modules\u00b6Python is a general-purpose programming language and is not specialized for\nnumerical or statistical computation. The core modules that enable Python to\nstore and access data efficiently and that provide statistical algorithms are\nlocated in modules.  The most important are:\n\nNumPy (numpy) - provide the basic array block used throughout numerical\nPython\npandas (pandas) - provides DataFrames which are used to store \ndata in an easy-to-use format\nSciPy (scipy) - Basic statistics and random number generators. The most\nimportant submodule is scipy.stats\nmatplotlib (matplotlib) - graphics. The most important submodule is\nmatplotlib.pyplot.\nstatsmodels (statsmodels) - statistical models such as OLS. The most\nimportant submodules are statsmodels.api and statsmodels.tsa.api.\n\nBegin by importing the important modules.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Canonical Names\u00b6Use the as keyword to import the modules using their canonical names:\n\n\nModule\nCanonical Name\n\n\n\n\nnumpy\nnp\n\n\npandas\npd\n\n\nscipy\nsp\n\n\nscipy.stats\nstats\n\n\nmatplotlib.pyplot\nplt\n\n\nstatsmodels.api\nsm\n\n\nstatsmodels.tsa.api\ntsa\n\n\n\nImport the core modules using import module as canonical.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Importing individual functions\u00b6\nImport array, sqrt, log and exp from NumPy.\nImport OLS from statsmodels.regression.linear_model\nImport the stats module from scipy\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Import det\u00b6The determinant function is located at numpy.linalg.det. Access this function\nusing:\n\nnumpy\nnp\nBy importing linalg from numpy and accessing it from linalg\nBy directly importing the function\n\nYou can x in the setup code to call the function as func(x).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: A simple 2 by 2 array to use with det\nimport numpy as np\nx = np.array([[2,3],[1,2]])\nprint(x)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-3/"
    },
    {
      "title": "Lesson 4",
      "text": "Series and DataFrames\u00b6This lesson covers:\n\nConstructing pandas Series and DataFrames \n\n\nData\u00b6September 2018 prices (adjusted closing prices) for the S&P 500 EFT (SPY),\nApple (AAPL) and Google (GOOG) are listed below:\n\n\nDate\nSPY Price\nAAPL Price\nGOOG Price\n\n\n\n\nSept4\n289.81\n228.36\n1197.00\n\n\nSept5\n289.03\n226.87\n1186.48\n\n\nSept6\n288.16\n223.10\n1171.44\n\n\nSept7\n287.60\n221.30\n1164.83\n\n\nSept10\n288.10\n218.33\n1164.64\n\n\nSept11\n289.05\n223.85\n1177.36\n\n\nSept12\n289.12\n221.07\n1162.82\n\n\nSept13\n290.83\n226.41\n1175.33\n\n\nSept14\n290.88\n223.84\n1172.53\n\n\nSept17\n289.34\n217.88\n1156.05\n\n\nSept18\n290.91\n218.24\n1161.22\n\n\nSept19\n291.44\n216.64\n1158.78\n\n\n\nPrices in September 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem: Input a pandas Series\u00b6Create vectors for each of the days in the Table named sep_xx\nwhere xx is the numeric date. For example,\nimport pandas as pd\n\nsep_04 = pd.Series([289.81,228.36,1197.00], index=[\"SPY\",\"AAPL\",\"GOOG\"]);\n\nUsing the ticker names as the index of each series\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a Vector of Dates\u00b6Use the pandas function pd.to_datetime to convert a list of string dates to\na pandas DateTimeIndex, which can be used to set dates in other arrays.\nFor example, the first two dates are\nimport pandas as pd\n\ndates_2 = pd.to_datetime([\"4-9-2018\",\"5-9-2018\"])\nprint(dates_2)\n\nwhich produces\n\nDatetimeIndex([\"2018-04-09\", \"2018-05-09\"], dtype=\"datetime64[ns]\", freq=None)\nCreate a vector containing all of the dates in the table.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Input a Series with Dates\u00b6Create vectors for each of the ticker symbols in Table named\nspy, aapl and goog, respectively. Use the variable dates that you created\nin the previous step as the index.\nFor example\ngoog = pd.Series([1197.00,1186.48,1171.44,...], index=dates)\n\nSet the name of each series as the series\" ticker.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Create a DataFrame\u00b6Create a DataFrame named prices containing Table. Set the\ncolumn names equal to the ticker and set the index to dates.\nprices = pd.DataFrame([[289.81, 228.36, 1197.00], [289.03, 226.87, 1186.48]],\n                      columns = [\"SPY\", \"AAPL\", \"GOOG\"],index=dates_2)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nSave the price data\nThis block saves prices to a HDF file for use in later lessons. The\nfunction used to save the data is covered in a later lesson.\nThis function uses some sophisticated features of Python. Do not\nworry if it is unclear at this point.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Save prices, goog and sep_04 into a single file for use in other lessons\n\n# Only run if prices has been defined\nif \"prices\" in globals():\n    import pandas as pd\n    dates = pd.Series(dates)\n    variables = [\"sep_04\", \"sep_05\", \"sep_06\", \"sep_07\", \"sep_10\", \"sep_11\",\n                 \"sep_12\", \"sep_13\", \"sep_14\", \"sep_17\", \"sep_18\", \"sep_19\",\n                 \"spy\", \"goog\", \"aapl\", \"prices\", \"dates\"]\n    with pd.HDFStore(\"data/dataframes.h5\", mode=\"w\") as h5:\n        for var in variables:\n            h5.put(var, globals()[var])\n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Creating DataFrames\u00b6Turn the table below into a DataFrame where the index is set as the index and\nthe column names are used in the DataFrame.\n\n\nindex\nFirm\nProfit\n\n\n\n\nA\nAlcoa\n3,428\n\n\nB\nBerkshire\n67,421\n\n\nC\nCoca Cola\n197.4\n\n\nD\nDannon\n-342.1\n\n\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-4/"
    },
    {
      "title": "Lesson 5",
      "text": "Constructing DataFrames from Series\u00b6This lesson introduced method to construct a DataFrame from multiple\nSeries.\nThis first block loads the variables created in an earlier lesson.  A\nlater lesson will cover loading and saving data.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load data created in an earlier lesson\n\nimport pandas as pd\n\nhdf_file = \"data/dataframes.h5\"\n\nsep_04 = pd.read_hdf(hdf_file, \"sep_04\")\nsep_05 = pd.read_hdf(hdf_file, \"sep_05\")\nsep_06 = pd.read_hdf(hdf_file, \"sep_06\")\nsep_07 = pd.read_hdf(hdf_file, \"sep_07\")\nsep_10 = pd.read_hdf(hdf_file, \"sep_10\")\nsep_11 = pd.read_hdf(hdf_file, \"sep_11\")\nsep_12 = pd.read_hdf(hdf_file, \"sep_12\")\nsep_13 = pd.read_hdf(hdf_file, \"sep_13\")\nsep_14 = pd.read_hdf(hdf_file, \"sep_14\")\nsep_17 = pd.read_hdf(hdf_file, \"sep_17\")\nsep_18 = pd.read_hdf(hdf_file, \"sep_18\")\nsep_19 = pd.read_hdf(hdf_file, \"sep_19\")\n\nspy = pd.read_hdf(hdf_file, \"spy\")\naapl = pd.read_hdf(hdf_file, \"aapl\")\ngoog = pd.read_hdf(hdf_file, \"goog\")\n\ndates = pd.to_datetime(pd.read_hdf(hdf_file, \"dates\"))\n\nprices = pd.read_hdf(hdf_file, \"prices\")\n\n\n    \n\n\n\n\n\n\n\nProblem: Construct a DataFrame from rows\u00b6Create a DataFrame named prices_row from the row vectors previously\nentered such that the results are identical to prices. For example, the first\ntwo days worth of data are:\ndates_2 = pd.to_datetime([\"1998-09-04\", \"1998-09-05\"])\nprices_row = pd.DataFrame([sep_04, sep_05])\n# Set the index after using concat to join\nprices_row.index = dates_2\n\nVerify that the DataFrame identical by printing the difference with\nprices\nprint(prices_row - prices)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Construct a DataFrame from columns\u00b6Create a DataFrame named prices_col from the 3 column vectors entered\nsuch that the results are identical to prices.\nNote: .T transposes a 2-d array since DataFrame builds the\narray by rows.\nVerify that the DataFrame identical by printing the difference with\nprices\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Construct a DataFrame from a dictionary\u00b6Create a DataFrame named prices_dict from the 3 column vectors entered\nsuch that the results are identical to prices\nVerify that the DataFrame identical by printing the difference with\nprices\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Create a DataFrame from rows\u00b6Use the three series populated below to create a DataFrame using each\nas a row.\nNote: Notice what happens in the resulting DataFrame since one of the\nSeries has 4 elements while the others have 3.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Data for the Exercises\nimport pandas as pd\nindex = [\"Num\", \"Let\", \"Date\"]\na = pd.Series([1, \"A\", pd.Timestamp(2018,12,31)], name=\"a\", index=index)\nb = pd.Series([2, \"B\", pd.Timestamp(2018,12,31)], name=\"b\", index=index)\nindex = [\"Num\", \"Let\", \"Date\", \"Float\"]\nc = pd.Series([3, \"C\", pd.Timestamp(2018,12,31), 3.0], name=\"c\", index=index)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Build a DataFrame from Columns\u00b6Build a DataFrame from the three series where each is used as a column.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-5/"
    },
    {
      "title": "Lesson 6",
      "text": "Methods and Functions\u00b6This lesson covers:\n\nCalling functions with more than one input and output \nCalling functions when some inputs are not used\n\n\n\n\n\n\n\n\nRead the data in momentum.csv and creating some variable. This cell uses\nsome magic to automate repeated typing.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\nimport pandas as pd\n\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\n\nprint(momentum.head())\n\nmom_01 = momentum[\"mom_01\"]\nmom_10 = momentum[\"mom_10\"]\n\n\n    \n\n\n\n\n\n\n\nThis data set contains 2 years of data on the 10 momentum portfolios from\n2016\u20132018. The variables are named mom_XX where XX ranges from 01 (work\nreturn over the past 12 months) to 10 (best return over the past 12 months).\n\n\n\n\n\n\n\nProblem: Calling Methods\u00b6Get used to calling methods by computing the mean, standard deviation, skewness, kurtosis, max, and min.\nUse the DataFrame functions mean, std, skew and kurt, min and max to print the\nvalues for mom_01.\nIn the second cell, call describe, a method that summarizes Series and DataFrames on mom_01.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Use NumPy and SciPy functions\u00b6Use the NumPy functions mean, std, min, max and the SciPy stats functions\nskew and kurtosis to produce the same output.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Calling Functions with 2 Outputs\u00b6Some useful functions return 2 or more outputs. One example is np.linalg.slogdet \ncomputes the signed log determinant of a square array. It returns two output,\nthe sign and the log of the absolute determinant.\nUse this function to compute the sign and log determinant of the 2 by 2 array:\n\n1  2\n2  9\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Calling Functions with 2 Inputs\u00b6Many functions take two or more inputs. Like outputs, the inputs are simply\nlisted in order separated by commas. Use np.linspace to produce a series\nof 11 points evenly spaced between 0 and 1.\n\nnp.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Calling Functions using Keyword Arguments\u00b6Many functions have optional arguments. You can see these in a docstring since\noptional arguments take the form variable=default. For example, see\nthe help for scipy.special.comb, which has the function signature\n\ncomb(N, k, exact=False, repetition=False)\nThis tells us that N and k are required and\nthat the other 2 inputs can be omitted if you are happy with the defaults.\nHowever, if we want to change some of the optional inputs, then we can\ndirectly use the inputs name in the function call.\nCompute the number of distinct combinations of 5 objects from a set of 10.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nCompute the total number of combinations allowing for repetition \nusing the repetition=True keyword argument.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nCompute the number of combinations using the exact representation using \nonly positional arguments for all 3 inputs.  Repeat using the keyword\nargument for exact.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Function Help\u00b6Explore the help available for calling functions ? operator. For example,\nimport scipy.stats as stats\n\nstats.kurtosis?\n\nopens a help window that shows the inputs and output, while\nhelp(stats.kurtosis)\n\nshows the help in the console.\nNote: VS Code does not support the ? form of help\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Use help with a method\u00b6Use help to get the help for the kurt method attached to momentum.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Use info\u00b6Use the info method on momentum to get information about this DataFrame.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Compute the day-by-day mean\u00b6Compute the day-by-day mean return of the portfolios in the momentum DataFrame using\nthe axis keyword argument. Use head and tail to show\nthe first 5 rows and last 5 rows\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Compute the standard deviation of mean returns\u00b6Compute the standard deviation of the mean returns by chaining methods.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Compute the average standard deviation\u00b6Compute the mean standard deviation as:\n$$ \\sqrt{N{-1} \\sum_{i=1}N V[r_i]} $$where $V[r_i]$ is the variance of portfolio $i$.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-6/"
    },
    {
      "title": "Lesson 7",
      "text": "Custom Functions\u00b6This lesson covers:\n\nWriting a custom function \n\n\n\n\n\n\n\n\nProblem: Writing a Custom Function\u00b6Custom functions will play an important role later in the course when\nestimating parameters. Construct a custom function that takes two arguments,\nmu and sigma2 and computes the likelihood function of a normal random variable.\n$$f(x;\\mu,\\sigma{2})=\\frac{1}{\\sqrt{2\\pi\\sigma{2}}}\\exp\\left(-\\frac{(x-\\mu){2}}{2\\sigma{2}}\\right)$$Use def to start the function and compute the likelihood of:\n$$x=0,\\mu=0,\\sigma{2}=1.$$The text in the triple quotes is the docstring which is optional.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Custom Function\u00b6Write a function named summary_stats that will take a single input, x,\na DataFrame and return a DataFrame with 4 columns and as many rows as\nthere were columns in the original data where the columns contain the mean,\nstandard deviation, skewness and kurtosis of x.\nCheck your function by running\nsummary_stats(momentum)\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load the momentum data\nimport pandas as pd\nmomentum = pd.read_csv(\"data\\momentum.csv\",index_col=\"date\", parse_dates=True)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nTest your function using the momentum data in the next cell.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Custom Function\u00b6Change your previous function to return 4 outputs, each a pandas Series for the mean,\nstandard deviation, skewness, and the kurtosis.\nReturning multiple outputs uses the syntax\nreturn w, x, y, z\n\nTest your function using the momentum data.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nTest your function using the momentum data in the next cell.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-7/"
    },
    {
      "title": "Lesson 8",
      "text": "Using DataFrames\u00b6This lesson introduces:\n\nComputing returns (percentage change)\nBasic mathematical operations on DataFrames\n\nThis first cell load data for use in this lesson.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load prices\nimport pandas as pd\nprices = pd.read_hdf(\"data/dataframes.h5\", \"prices\")\nsep_04 = pd.read_hdf(\"data/dataframes.h5\", \"sep_04\")\ngoog = pd.read_hdf(\"data/dataframes.h5\", \"goog\")\n\n\n    \n\n\n\n\n\n\n\nProblem: Compute Returns\u00b6Compute returns using\nreturns = prices.pct_change()\n\nwhich computes the percentage change.\nAdditionally, extract returns for each name using\nspy_returns = returns[\"SPY\"]\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Compute Log Returns\u00b6import numpy as np\n\nlog_returns = np.log(prices).diff()\n\nfirst difference of the natural log of the prices. Mathematically this is \n$r_{t}=\\ln\\left(P_{t}\\right)-\\ln\\left(P_{t-1}\\right)=\\ln\\left(\\frac{P_{t}}{P_{t-1}}\\right)\\approx\\frac{P_{t}}{P_{t-1}}-1$.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nBasic Mathematical Operations\u00b6\n\nOperation\nSymbol\nPrecedence\n\n\n\n\nParentheses\n()\n4\n\n\nExponentiation\n**\n3\n\n\nMultiplication\n*\n2\n\n\nDivision\n/\n2\n\n\nFloor division\n//\n2\n\n\nModulus\n%\n2\n\n\nMatrix multiplication\n@\n2\n\n\nAddition\n+\n1\n\n\nSubtraction\n-\n1\n\n\n\nNote: Higher precedence operators are evaluated first, and ties are\nevaluated left to right.\nProblem: Scalar Operations\u00b6\nAdd 1 to all returns\nSquare the returns\nMultiply the price of Google by 2. \nExtract the fractional return using floor division and modulus\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Addition of Series\u00b6Add the returns on SPY to those of AAPL\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Combining methods and mathematical operations\u00b6Using only basic mathematical operations compute the \ncorrelation between the returns on AAPL and SPY.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Addition of DataFrames\u00b6Construct a DataFrame that only contains the SPY column from returns\nand add it to the return DataFrame\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Non-conformable math\u00b6Add the prices in sep_04 to the prices of goog. What happens?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Constructing portfolio returns\u00b6Set up a 3-element array of portfolio weights\n$$w=\\left(\\frac{1}{3},\\,\\frac{1}{3}\\,,\\frac{1}{3}\\right)$$and compute the return of a portfolio with weight $\\frac{1}{3}$ in each security.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Combine math with function\u00b6Add 1 to the output of np.arange to produce the sequence 1, 2, ..., 10.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Understand pandas math\u00b6Use the Series and DataFrame below to compute the sums\n\na+b\na+c\nb+c\na+b+c\n\nto understand how missing values are treated by pandas\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Data for exercise\nimport pandas as pd\nimport numpy as np\n\nrs = np.random.RandomState(19991231)\n\nidx = [\"A\",\"a\",\"B\",3]\ncolumns = [\"A\",1,\"B\",3]\na = pd.Series([1,2,3,4], index=idx)\nb = pd.Series([10,9,8,7], index=columns)\nvalues = rs.randint(1, 11, size=(4,4))\nc = pd.DataFrame(values, columns=columns, index=idx)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Math with duplicates\u00b6Add the Series d to a to see what happens with delays.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Data for exercise\n\nd = pd.Series([10, 101], index=[\"A\",\"A\"])\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-8/"
    },
    {
      "title": "Lesson 9",
      "text": "Common DataFrame methods\u00b6This lesson introduces the common DataFrame methods that\nwe will repeatedly use in the course.\nThis first cell load data for use in this lesson.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load prices\nimport pandas as pd\nprices = pd.read_hdf(\"data/dataframes.h5\", \"prices\")\nsep_04 = pd.read_hdf(\"data/dataframes.h5\", \"sep_04\")\ngoog = pd.read_hdf(\"data/dataframes.h5\", \"goog\")\nreturns = prices.pct_change().dropna()\nspy_returns = returns.SPY\naapl_returns = returns.AAPL\ngoog_returns = returns.GOOG\n\n\n    \n\n\n\n\n\n\n\nProblem: Constructing portfolio returns\u00b6Compute the return of a portfolio with weight $\\frac{1}{3}$ in each security using\nmultiplication (*) and .sum().\nNote: You need to use the axis keyword for the sum.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Compute the Mean and Standard Deviation\u00b6Using the function mean, compute the mean of the three returns series one at a time. For example\ngoog_mean = goog_returns.mean()\n\nNext, compute the mean of the matrix of returns using\nretmean = returns.mean()\n\nWhat is the relationship between these two? Repeat this exercise for the standard deviation (std()).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Compute Correlation\u00b6Compute the correlation of the matrix of returns (corr()).\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Summing all elements\u00b6Compute the sum of the columns of returns using .sum(). How is this related to the mean computed \nin the previous step?\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Maximum and Minimum Values\u00b6Compute the minimum and maximum values of the columns of returns using the min() and max() commands.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nProblem: Rounding Up, Down and to the Closest Integer\u00b6Rounding up is handled by ceil, rounding down is handled by floor and rounding to the closest \ninteger is handled by round. Try all of these commands on 100 times returns. For example,\nrounded = (100*returns).round()\n\nUse ceil and floor to round up and down, respectively.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercises\u00b6Exercise: Compute Quantiles\u00b6Compute the 5%, 25%, 50%, 75% and 95% quantiles of momentum using the quantile\nmethod.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Setup: Load data\nimport pandas as pd\nmomentum = pd.read_csv(\"data/momentum.csv\", index_col=\"date\", parse_dates=True)\nmom_10 = momentum.mom_10\n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Sorting\u00b6Use sort_values to sort momentum by the column mom_10. Verify that the\nsort was successful by looking at the minimum of a diff.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Sort Descending\u00b6Use sort_values to sort momentum by by the column mom_10 using a descending\nsort (see the help for sort_values). Verify the sort worked by looking at the maximum of\na diff.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Get Number of Elements\u00b6Use the shape property to get the number of observations in momentum. Use it\nagain to get the number of columns.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n \n\n\n    \n\n\n\n\n\n\n\nExercise: Use shift to Compute Returns\u00b6Compute the percentage change using only shift, division (/) and\nsubtraction (-) on the Series mom_10. Verify that your result matches what pct_change produces.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/lesson-9/"
    },
    {
      "title": "Example: Fama-MacBeth regression",
      "text": "Estimating the Risk Premia using Fama-MacBeth Regressions\u00b6\n\n\n\n\n\n\nThis example highlights how to implement a Fama-MacBeth 2-stage regression to estimate factor risk premia, make inference on the risk premia, and test whether a linear factor model can explain a cross-section of portfolio returns. This example closely follows [Cochrane::2001] (See also [JagannathanSkoulakisWang::2010]). As in the previous example, the first segment contains the imports.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nfrom numpy import mat, cov, mean, hstack, multiply,sqrt,diag, \\\n    squeeze, ones, array, vstack, kron, zeros, eye, savez_compressed\nfrom numpy.linalg import inv\nfrom scipy.stats import chi2\nfrom pandas import read_csv\nimport statsmodels.api as sm\n\n\n    \n\n\n\n\n\n\n\nNext, the data are imported. I formatted the data downloaded from Ken French's website into an easy-to-import CSV which can be read by pandas.read_csv. The data is split using named columns for the small sets of variables and ix for the portfolios. The code uses pure NumPy arrays, and so values is used to retrieve the array from the DataFrame. The dimensions are determined using shape. Finally the risk free rate is forced to have 2 dimensions so that it will be broadcastable with the portfolio returns in the construction of the excess returns to the Size and Value-weighted portfolios. asmatrix is used to return matrix views of all of the arrays. This code is linear algebra-heavy and so matrices are easier to use than arrays.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ndata = read_csv('FamaFrench.csv')\n\n# Split using both named colums and ix for larger blocks\ndates = data['date'].values\nfactors = data[['VWMe', 'SMB', 'HML']].values\nriskfree = data['RF'].values\nportfolios = data.iloc[:, 5:].values\n\n# Use mat for easier linear algebra\nfactors = mat(factors)\nriskfree = mat(riskfree)\nportfolios = mat(portfolios)\n\n# Shape information\nT,K = factors.shape\nT,N = portfolios.shape\n# Reshape rf and compute excess returns\nriskfree.shape = T,1\nexcessReturns = portfolios - riskfree\n\n\n    \n\n\n\n\n\n\n\nThe next block does 2 things:\n\nCompute the time-series $\\beta$s. This is done be regressing the full array of excess returns on the factors (augmented with a constant) using lstsq.\nCompute the risk premia using a cross-sectional regression of average excess returns on the estimates $\\beta$s. This is a standard regression where the step 1 $\\beta$ estimates are used as regressors, and the dependent variable is the average excess return.\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n# Time series regressions\nX = sm.add_constant(factors)\nts_res = sm.OLS(excessReturns, X).fit()\nalpha = ts_res.params[0]\nbeta = ts_res.params[1:]\navgExcessReturns = mean(excessReturns, 0)\n# Cross-section regression\ncs_res = sm.OLS(avgExcessReturns.T, beta.T).fit()\nriskPremia = cs_res.params\n\n\n    \n\n\n\n\n\n\n\nThe asymptotic variance requires computing the covariance of the demeaned returns and the weighted pricing errors. The problem is formulated using 2-step GMM where the moment conditions are \n\\begin{equation}\ng_{t}\\left(\\theta\\right)=\\left[\\begin{array}{c}\n\\epsilon_{1t}\\\\\n\\epsilon_{1t}f_{t}\\\\\n\\epsilon_{2t}\\\\\n\\epsilon_{2t}f_{t}\\\\\n\\vdots\\\\\n\\epsilon_{Nt}\\\\\n\\epsilon_{Nt}f_{t}\\\\\n\\beta u_{t}\n\\end{array}\\right]\n\\end{equation}\nwhere $\\epsilon_{it}=r_{it}{e}-\\alpha_{i}-\\beta_{i}{\\prime}f_{t}$, $\\beta_{i}$ is a $K$ by 1 vector of factor loadings, $f_{t}$ is a $K$ by 1 set of factors, $\\beta=\\left[\\beta_{1}\\,\\beta_{2}\\ldots\\beta_{N}\\right]$ is a $K$ by $N$ matrix of all factor loadings, $u_{t}=r_{t}{e}-\\beta'\\lambda$ are the $N$ by 1 vector of pricing errors and $\\lambda$ is a $K$  by 1 vector of risk premia. \nThe vector of parameters is then $\\theta= \\left[\\alpha_{1}\\:\\beta_{1}{\\prime}\\:\\alpha_{2}\\:\\beta_{2}{\\prime}\\:\\ldots\\:\\alpha_{N}\\,\\beta_{N}{\\prime}\\:\\lambda'\\right]'$\n To make inference on this problem, the derivative of the moments with respect to the parameters, $\\partial g_{t}\\left(\\theta\\right)/\\partial\\theta{\\prime}$ is needed. With some work, the estimator of this matrix can be seen to be\n\\begin{equation}\n G=E\\left[\\frac{\\partial g_{t}\\left(\\theta\\right)}{\\partial\\theta{\\prime}}\\right]=\\left[\\begin{array}{cc}\n-I_{n}\\otimes\\Sigma_{X} & 0\\\\\nG_{21} & -\\beta\\beta{\\prime}\n\\end{array}\\right].\n\\end{equation}where $X_{t}=\\left[1\\: f_{t}{\\prime}\\right]'$  and $\\Sigma_{X}=E\\left[X_{t}X_{t}{\\prime}\\right]$. $G_{21}$ is a matrix with the structure\n\\begin{equation}\nG_{21}=\\left[G_{21,1}\\, G_{21,2}\\,\\ldots G_{21,N}\\right]\n\\end{equation}where\n\\begin{equation}\nG_{21,i}=\\left[\\begin{array}{cc} \n0_{K,1} & \\textrm{diag}\\left(E\\left[u_{i}\\right]-\\beta_{i}\\odot\\lambda\\right)\\end{array}\\right]\\end{equation}and where $E\\left[u_{i}\\right]$ is the expected pricing error. In estimation, all expectations are replaced with their sample analogues.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \n# Moment conditions\nX = sm.add_constant(factors)\np = vstack((alpha, beta))\nepsilon = excessReturns - X @ p\nmoments1 = kron(epsilon, ones((1, K + 1)))\nmoments1 = multiply(moments1, kron(ones((1, N)), X))\nu = excessReturns - riskPremia[None,:] @ beta\nmoments2 = u * beta.T\n# Score covariance\nS = mat(cov(hstack((moments1, moments2)).T))\n# Jacobian\nG = mat(zeros((N * K + N + K, N * K + N + K)))\nSigmaX = (X.T @ X) / T\nG[:N * K + N, :N * K + N] = kron(eye(N), SigmaX)\nG[N * K + N:, N * K + N:] = -beta @ beta.T\nfor i in range(N):\n    temp = zeros((K, K + 1))\n    values = mean(u[:, i]) - multiply(beta[:, i], riskPremia)\n    temp[:, 1:] = diag(values)\n    G[N * K + N:, i * (K + 1):(i + 1) * (K + 1)] = temp\n\nvcv = inv(G.T) * S * inv(G) / T\n\n\n    \n\n\n\n\n\n\n\nThe $J$-test examines whether the average pricing errors, $\\hat{\\alpha}$, are zero. The $J$ statistic has an asymptotic $\\chi_{N}{2}$  distribution, and the model is badly rejected.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nvcvAlpha = vcv[0:N * K + N:4, 0:N * K + N:4]\nJ = alpha @ inv(vcvAlpha) @ alpha.T\nJ = J[0, 0]\nJpval = 1 - chi2(25).cdf(J)\n\n\n    \n\n\n\n\n\n\n\nThe final block using formatted output to present all of the results in a readable manner.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nvcvRiskPremia = vcv[N * K + N:, N * K + N:]\nannualizedRP = 12 * riskPremia\narp = list(squeeze(annualizedRP))\narpSE = list(sqrt(12 * diag(vcvRiskPremia)))\nprint('        Annualized Risk Premia')\nprint('           Market       SMB        HML')\nprint('--------------------------------------')\nprint('Premia     {0:0.4f}    {1:0.4f}     {2:0.4f}'.format(arp[0], arp[1], arp[2]))\nprint('Std. Err.  {0:0.4f}    {1:0.4f}     {2:0.4f}'.format(arpSE[0], arpSE[1], arpSE[2]))\nprint('\\n\\n')\n\nprint('J-test:   {:0.4f}'.format(J))\nprint('P-value:   {:0.4f}'.format(Jpval))\n\ni = 0\nbetaSE = []\nfor j in range(5):\n    for k in range(5):\n        a = alpha[i]\n        b = beta[:, i]\n        variances = diag(vcv[(K + 1) * i:(K + 1) * (i + 1), (K + 1) * i:(K + 1) * (i + 1)])\n        betaSE.append(sqrt(variances))\n        s = sqrt(variances)\n        c = hstack((a, b))\n        t = c / s\n        print('Size: {:}, Value:{:}   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)'.format(j + 1, k + 1))\n        print('Coefficients: {:>10,.4f}  {:>10,.4f}  {:>10,.4f}  {:>10,.4f}'.format(a, b[0], b[1], b[2]))\n        print('Std Err.      {:>10,.4f}  {:>10,.4f}  {:>10,.4f}  {:>10,.4f}'.format(s[0], s[1], s[2], s[3]))\n        print('T-stat        {:>10,.4f}  {:>10,.4f}  {:>10,.4f}  {:>10,.4f}'.format(t[0], t[1], t[2], t[3]))\n        print('')\n        i += 1\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n        Annualized Risk Premia\n           Market       SMB        HML\n--------------------------------------\nPremia     6.6642    2.8731     2.8080\nStd. Err.  0.5994    0.4010     0.4296\n\n\n\nJ-test:   95.2879\nP-value:   0.0000\nSize: 1, Value:1   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.8354      1.3099      1.2892      0.3943\nStd Err.          0.1820      0.1269      0.1671      0.2748\nT-stat           -4.5904     10.3196      7.7127      1.4348\n\nSize: 1, Value:2   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.3911      1.0853      1.6100      0.3317\nStd Err.          0.1237      0.0637      0.1893      0.1444\nT-stat           -3.1616     17.0351      8.5061      2.2971\n\nSize: 1, Value:3   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.1219      1.0747      1.1812      0.4648\nStd Err.          0.0997      0.0419      0.0938      0.0723\nT-stat           -1.2225     25.6206     12.5952      6.4310\n\nSize: 1, Value:4   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0388      0.9630      1.2249      0.5854\nStd Err.          0.0692      0.0232      0.1003      0.0353\nT-stat            0.5614     41.5592     12.2108     16.5705\n\nSize: 1, Value:5   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0918      0.9850      1.3453      0.9052\nStd Err.          0.0676      0.0255      0.0818      0.0610\nT-stat            1.3580     38.5669     16.4489     14.8404\n\nSize: 2, Value:1   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.2397      1.0691      1.0520     -0.2647\nStd Err.          0.0725      0.0318      0.0609      0.0591\nT-stat           -3.3052     33.6540     17.2706     -4.4768\n\nSize: 2, Value:2   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.0194      1.0416      0.9880      0.1877\nStd Err.          0.0615      0.0170      0.0776      0.0350\nT-stat           -0.3162     61.1252     12.7393      5.3646\n\nSize: 2, Value:3   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0898      0.9590      0.8619      0.3553\nStd Err.          0.0517      0.0170      0.0733      0.0320\nT-stat            1.7359     56.4856     11.7528     11.0968\n\nSize: 2, Value:4   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0482      0.9788      0.8178      0.5562\nStd Err.          0.0495      0.0138      0.0454      0.0281\nT-stat            0.9733     70.7006     18.0210     19.8055\n\nSize: 2, Value:5   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.0109      1.0502      0.9373      0.8493\nStd Err.          0.0596      0.0182      0.0281      0.0263\nT-stat           -0.1830     57.7092     33.3971     32.2980\n\nSize: 3, Value:1   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.1556      1.1416      0.7883     -0.1980\nStd Err.          0.0591      0.0190      0.0445      0.0411\nT-stat           -2.6320     60.1173     17.6973     -4.8171\n\nSize: 3, Value:2   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0889      1.0133      0.5151      0.0720\nStd Err.          0.0553      0.0179      0.0340      0.0334\nT-stat            1.6068     56.6380     15.1651      2.1546\n\nSize: 3, Value:3   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.1118      1.0129      0.4130      0.3379\nStd Err.          0.0578      0.0267      0.0324      0.0321\nT-stat            1.9344     37.9790     12.7488     10.5399\n\nSize: 3, Value:4   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0818      0.9615      0.4646      0.5068\nStd Err.          0.0568      0.0141      0.0475      0.0301\nT-stat            1.4399     68.3360      9.7754     16.8580\n\nSize: 3, Value:5   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.0526      1.1447      0.4970      0.9143\nStd Err.          0.0687      0.0197      0.0509      0.0390\nT-stat           -0.7655     58.0319      9.7690     23.4302\n\nSize: 4, Value:1   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0902      1.0661      0.2857     -0.3692\nStd Err.          0.0498      0.0151      0.0444      0.0323\nT-stat            1.8127     70.4710      6.4268    -11.4334\n\nSize: 4, Value:2   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.0104      1.0308      0.2430      0.1328\nStd Err.          0.0534      0.0217      0.0300      0.0294\nT-stat           -0.1952     47.5567      8.0926      4.5183\n\nSize: 4, Value:3   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0392      1.0096      0.2214      0.2980\nStd Err.          0.0572      0.0209      0.0436      0.0486\nT-stat            0.6862     48.3271      5.0836      6.1333\n\nSize: 4, Value:4   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0148      1.0437      0.2016      0.5857\nStd Err.          0.0593      0.0224      0.0343      0.0484\nT-stat            0.2497     46.5053      5.8694     12.0922\n\nSize: 4, Value:5   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.1762      1.2284      0.2974      0.9834\nStd Err.          0.0803      0.0224      0.0490      0.0378\nT-stat           -2.1927     54.8427      6.0726     26.0265\n\nSize: 5, Value:1   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0794      1.0310     -0.1507     -0.2508\nStd Err.          0.0372      0.0095      0.0247      0.0168\nT-stat            2.1369    108.0844     -6.1067    -14.9673\n\nSize: 5, Value:2   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:     0.0535      0.9576     -0.1893     -0.0107\nStd Err.          0.0457      0.0170      0.0243      0.0239\nT-stat            1.1690     56.3228     -7.7765     -0.4458\n\nSize: 5, Value:3   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.0236      0.9753     -0.2173      0.3127\nStd Err.          0.0559      0.0178      0.0309      0.0256\nT-stat           -0.4225     54.6936     -7.0217     12.2061\n\nSize: 5, Value:4   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -0.1978      1.0546     -0.1732      0.7115\nStd Err.          0.0587      0.0230      0.0300      0.0316\nT-stat           -3.3679     45.7933     -5.7749     22.5339\n\nSize: 5, Value:5   Alpha   Beta(VWM)   Beta(SMB)   Beta(HML)\nCoefficients:    -1.2737      1.1045      0.0076      0.8527\nStd Err.          0.3557      0.1143      0.1594      0.1490\nT-stat           -3.5805      9.6657      0.0477      5.7232\n\n\n\n\n\n\n\n\n\n\n\n\nThe final block converts the standard errors of $\\beta$ to be an array and saves the results.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nbetaSE = array(betaSE)\nsavez_compressed('fama-macbeth-results', alpha=alpha, beta=beta,\n                 betaSE=betaSE, arpSE=arpSE, arp=arp, J=J, Jpval=Jpval)\n\n\n    \n\n\n\n\n\n\n\nSave Results\u00b6Save the estimated values for use in the $\\LaTeX$ notebook.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nfrom numpy import savez\nsavez('fama-macBeth-results.npz', arp=arp, beta=beta, arpSE=arpSE,\n      betaSE=betaSE, J=J, Jpval=Jpval)",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/notebooks/example-fama-macbeth/"
    },
    {
      "title": "Example: GJR-GARCH Estimation",
      "text": "IPython Notebook Setup\u00b6This commands are used needed for plots to appear in the notebook.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n%matplotlib inline\n\n\n    \n\n\n\n\n\n\n\nEstimating the Parameters of a GJR-GARCH Model\u00b6This example will highlight the steps needed to estimate the parameters of a GJR-GARCH(1,1,1) model with a constant mean. The volatility dynamics in a GJR-GARCH model are given by \n$$\\sigma_{t}{2}=\\omega+\\sum_{i=1}{p}\\alpha_{i}\\epsilon_{t-i}{2}+\\sum_{j=1}{o}\\gamma_{j}r_{t-j}{2}I_{\\left[\\epsilon_{t-j}<0\\right]}+\\sum_{k=1}{q}\\beta_{k}\\sigma_{t-k}{2}.$$\nReturns are assumed to be conditionally normal, $r_{t}|\\mathcal{F}_{t-1}\\sim N\\left(\\mu,\\sigma_{t}{2}\\right)$, $\\epsilon_{t}=r_{t}-\\mu$ and parameters are estimated by maximum likelihood. To estimate the parameters, it is necessary to:\n\nProduce some starting values\nEstimate the parameters using (quasi-) maximum likelihood\nCompute standard errors using a \u201csandwich\u201d covariance estimator (also known as the [BollerslevWooldridge::1992] covariance estimator)\n\nThe first task is to write the log-likelihood which can be used in an optimizer. The log-likelihood function will compute the volatility recursion and the log-likelihood. It will also, optionally, return the $T$ by 1 vector of individual log-likelihoods which are useful when approximating the scores.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpy import size, log, pi, sum, array, zeros, diag, mat, asarray, sqrt, \\\n    copy\nfrom numpy.linalg import inv\nfrom scipy.optimize import fmin_slsqp\n\n\n    \n\n\n\n\n\n\n\nThe conditional log-likelihood of a normal random variable is\n$$\\ln f\\left(r_{t}|\\mu,\\sigma_{t}{2}\\right)=-\\frac{1}{2}\\left(\\ln2\\pi+\\ln\\sigma_{t}{2}+\\frac{\\left(r_{t}-\\mu\\right){2}}{\\sigma_{t}{2}}\\right),$$which is negated in the code since the optimizers all minimize.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndef gjr_garch_likelihood(parameters, data, sigma2, out=None):\n    ''' Returns negative log-likelihood for GJR-GARCH(1,1,1) model.'''\n    mu = parameters[0]\n    omega = parameters[1]\n    alpha = parameters[2]\n    gamma = parameters[3]\n    beta = parameters[4]\n    \n    T = size(data,0)\n    eps = data - mu\n    # Data and sigma2 are T by 1 vectors\n    for t in range(1,T):\n        sigma2[t] = (omega + alpha * eps[t-1]**2 \n                     + gamma * eps[t-1]**2 * (eps[t-1]<0) + beta * sigma2[t-1])\n    \n    logliks = 0.5*(log(2*pi) + log(sigma2) + eps**2/sigma2)\n    loglik = sum(logliks)\n    \n    if out is None:\n        return loglik\n    else:\n        return loglik, logliks, copy(sigma2)\n\n\n    \n\n\n\n\n\n\n\nThe keyword argument out has a default value of None, and is used to determine whether to return 1 output or 3. This is common practice since the optimizer requires a single output -- the log-likelihood function value, but it is also useful to be able to output other useful quantities, such as $\\left\\{ \\sigma_{t}{2}\\right\\}$.\nThe optimization is constrained so that $\\alpha+\\gamma/2+\\beta\\leq 1$, and the constraint is provided in a separate function.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ndef gjr_constraint(parameters, data, sigma2, out=None):\n    ''' Constraint that alpha+gamma/2+beta<=1'''\n    \n    alpha = parameters[2]\n    gamma = parameters[3]\n    beta = parameters[4]\n\n    return array([1-alpha-gamma/2-beta])\n\n\n    \n\n\n\n\n\n\n\nNote that the constraint function takes the same inputs as the negative of the log-likelihood function, even though only parameters is required to compute the constraint.\nIt is necessary to discuss one other function before proceeding with the main block of code. The asymptotic variance is estimated using the \u201csandwich\u201d form which is commonly expressed as\n$$\\mathcal{J}{-1}\\mathcal{I}\\mathcal{J}{-1}$$where $\\mathcal{J}$ is the expected Hessian and $\\mathcal{I}$ is the covariance of the scores. Both are numerically approximated, and the strategy for computing the Hessian is to use the definition that\n$$\\mathcal{J}_{ij}\\approx\\frac{f\\left(\\theta+e_{i}h_{i}+e_{j}h_{j}\\right)-f\\left(\\theta+e_{i}h_{i}\\right)-f\\left(\\theta+e_{j}h_{j}\\right)+f\\left(\\theta\\right)}{h_{i}h_{j}}$$where $h_{i}$ is a scalar \u201cstep size\u201d and $e_{i}$ is a vector of 0s except for element $i$, which is 1. A 2-sided version of this approximation, which takes both forward and backward steps and then averages, is below. For more on numerical derivatives, see [FlanneryPressTeukolskyTeukolsky::1992].\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndef hessian_2sided(fun, theta, args):\n    f = fun(theta, *args)\n    h = 1e-5*np.abs(theta)\n    thetah = theta + h\n    h = thetah - theta\n    K = size(theta,0)\n    h = np.diag(h)\n    \n    fp = zeros(K)\n    fm = zeros(K)\n    for i in range(K):\n        fp[i] = fun(theta+h[i], *args)\n        fm[i] = fun(theta-h[i], *args)\n        \n    fpp = zeros((K,K))\n    fmm = zeros((K,K))\n    for i in range(K):\n        for j in range(i,K):\n            fpp[i,j] = fun(theta + h[i] + h[j],  *args)\n            fpp[j,i] = fpp[i,j]\n            fmm[i,j] = fun(theta - h[i] - h[j],  *args)\n            fmm[j,i] = fmm[i,j]\n            \n    hh = (diag(h))\n    hh = hh.reshape((K,1))\n    hh = hh @ hh.T\n    \n    H = zeros((K,K))\n    for i in range(K):\n        for j in range(i,K):\n            H[i,j] = (fpp[i,j] - fp[i] - fp[j] + f \n                       + f - fm[i] - fm[j] + fmm[i,j])/hh[i,j]/2\n            H[j,i] = H[i,j]\n    \n    return H\n\n\n    \n\n\n\n\n\n\n\nFinally, the code that does the actual work can be written. The first block imports the data, flips it using a slicing operator, and computes 100 times returns. Scaling data can be useful to improve optimizer performance, and ideally estimated parameters should have similar magnitudes (i.e. $\\omega\\approx.01$  and $\\alpha\\approx.05$).\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n# Import data\nFTSE = pd.read_csv('FTSE_1984_2012.csv', parse_dates=[0])\n# Set index\nFTSE.index = FTSE.pop('Date')\n# Flip upside down\nFTSE = FTSE.iloc[::-1]\n# Compute returns\nFTSEprice = FTSE['Adj Close']\nFTSEreturn = 100 * FTSEprice.pct_change().dropna()\n\n\n    \n\n\n\n\n\n\n\nGood starting values are important. These are my guesses based on experience fitting these types of models models. An alternative is to attempt a crude grid search and use the best (smallest) log-likelihood value from the grid search.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n# Starting values\nstartingVals = array([FTSEreturn.mean(),\n                      FTSEreturn.var() * .01,\n                      .03, .09, .90])\n\n\n    \n\n\n\n\n\n\n\nBounds are used in estimation to ensure that all parameters in the conditional variance are $\\geq 0$  and to set sensible upper bounds on the mean and $\\omega$. The vector sigma2 is then initialized, and the arguments are placed in a tuple.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n# Estimate parameters\nfinfo = np.finfo(np.float64)\nbounds = [(-10*FTSEreturn.mean(), 10*FTSEreturn.mean()),\n          (finfo.eps, 2*FTSEreturn.var() ),\n          (0.0,1.0), (0.0,1.0), (0.0,1.0)]\n       \nT = FTSEreturn.shape[0]\nsigma2 = np.ones(T) * FTSEreturn.var()\n# Pass a NumPy array, not a pandas Series\nargs = (np.asarray(FTSEreturn), sigma2)\nestimates = fmin_slsqp(gjr_garch_likelihood, startingVals,\n                       f_ieqcons=gjr_constraint, bounds = bounds,\n                       args = args)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 9569.030507603133\n            Iterations: 12\n            Function evaluations: 101\n            Gradient evaluations: 12\n\n\n\n\n\n\n\n\n\n\n\nThe optimized log-likelihood and the time series of variances are computed by calling the objective using the keyword argument out=True.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nloglik, logliks, sigma2final = gjr_garch_likelihood(estimates, FTSEreturn,\n                                                    sigma2, out=True)\n\n\n    \n\n\n\n\n\n\n\nNext, the numerical scores and the covariance of the scores are computed. These exploit the definition of a derivative, so that for a scalar function,\n$$\\frac{\\partial f\\left(\\theta\\right)}{\\partial\\theta_{i}}\\approx\\frac{f\\left(\\theta+e_{i}h_{i}\\right)-f\\left(\\theta\\right)}{h_{i}}.$$The covariance is computed as the outer product of the scores since the scores should have mean 0 when evaluated at the solution to the optimization problem.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nstep = 1e-5 * estimates\nscores = zeros((T,5))\nfor i in range(5):\n    h = step[i]\n    delta = np.zeros(5)\n    delta[i] = h\n    \n    loglik, logliksplus, sigma2 = gjr_garch_likelihood(estimates + delta, \\\n                               np.asarray(FTSEreturn), sigma2, out=True)\n    loglik, logliksminus, sigma2 = gjr_garch_likelihood(estimates - delta, \\\n                              np.asarray(FTSEreturn), sigma2, out=True)                   \n               \n    scores[:,i] = (logliksplus - logliksminus)/(2*h)\n\nI = (scores.T @ scores)/T\n\n\n    \n\n\n\n\n\n\n\nThe next block calls hessian_2sided to estimate the Hessian, and then computes the asymptotic covariance.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nJ = hessian_2sided(gjr_garch_likelihood, estimates, args)\nJ = J/T\nJinv = mat(inv(J))\nvcv = Jinv*mat(I)*Jinv/T\nvcv = asarray(vcv)\n\n\n    \n\n\n\n\n\n\n\nThe penultimate step is to pretty print the results and to produce a plot of the conditional variances.\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \noutput = np.vstack((estimates,sqrt(diag(vcv)),estimates/sqrt(diag(vcv)))).T    \nprint('Parameter   Estimate       Std. Err.      T-stat')\nparam = ['mu','omega','alpha','gamma','beta']\nfor i in range(len(param)):\n    print('{0:<11} {1:>0.6f}        {2:0.6f}    {3: 0.5f}'.format(param[i],\n           output[i,0], output[i,1], output[i,2]))\n    \n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nParameter   Estimate       Std. Err.      T-stat\nmu          0.032146        0.010084     3.18795\nomega       0.017610        0.003330     5.28813\nalpha       0.030658        0.006730     4.55564\ngamma       0.091709        0.012944     7.08484\nbeta        0.906327        0.009784     92.62951\n\n\n\n\n\n\n\n\n\n\n\nThis final block produces a plot of the annualized conditional standard deviations.\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \n# Register date converters\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n# Produce a plot\ndates = FTSE.index[1:]\nfig = plt.figure()\nax = fig.add_subplot(111)\nvolatility = pd.DataFrame(np.sqrt(252 * sigma2), index=dates)\nax.plot(volatility.index,volatility)\nax.autoscale(tight='x')\nfig.autofmt_xdate()\nfig.tight_layout(pad=1.5)\nax.set_ylabel('Volatility')\nax.set_title('FTSE Volatility (GJR GARCH(1,1,1))')\nplt.show()",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/notebooks/example-gjr-garch/"
    },
    {
      "title": "Example: GMM Estimation",
      "text": "Risk Premia Estimation using GMM\u00b6\n\n\n\n\n\n\nStart by importing the modules and functions needed\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nfrom numpy import hstack, ones, array, mat, tile, reshape, squeeze, eye, asmatrix\nfrom numpy.linalg import inv\nfrom pandas import read_csv, Series \nfrom scipy.linalg import kron\nfrom scipy.optimize import fmin_bfgs\nimport numpy as np\nimport statsmodels.api as sm\n\n\n    \n\n\n\n\n\n\n\nNext a callable function is used to produce iteration-by-iteration output when using the non-linear optimizer.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \niteration = 0\nlastValue = 0\nfunctionCount = 0\n\ndef iter_print(params):\n    global iteration, lastValue, functionCount\n    iteration += 1\n    print('Func value: {0:}, Iteration: {1:}, Function Count: {2:}'.format(lastValue, iteration, functionCount))\n\n\n    \n\n\n\n\n\n\n\nThe GMM objective, which is minimized, is defined next.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndef gmm_objective(params, pRets, fRets, Winv, out=False):\n    global lastValue, functionCount\n    T,N = pRets.shape\n    T,K = fRets.shape\n    beta = squeeze(array(params[:(N*K)]))\n    lam = squeeze(array(params[(N*K):]))\n    beta = reshape(beta,(N,K))\n    lam = reshape(lam,(K,1))\n    betalam = beta @ lam\n    expectedRet = fRets @ beta.T\n    e = pRets - expectedRet\n    instr = tile(fRets,N)\n    moments1  = kron(e,ones((1,K)))\n    moments1 = moments1 * instr\n    moments2 = pRets - betalam.T\n    moments = hstack((moments1,moments2))\n\n    avgMoment = moments.mean(axis=0)\n    \n    J = T * mat(avgMoment) * mat(Winv) * mat(avgMoment).T\n    J = J[0,0]\n    lastValue = J\n    functionCount += 1\n    if not out:\n        return J\n    else:\n        return J, moments\n\n\n    \n\n\n\n\n\n\n\nThe G matrix, which is the derivative of the GMM moments with respect to the parameters, is defined.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ndef gmm_G(params, pRets, fRets):\n    T,N = pRets.shape\n    T,K = fRets.shape\n    beta = squeeze(array(params[:(N*K)]))\n    lam = squeeze(array(params[(N*K):]))\n    beta = reshape(beta,(N,K))\n    lam = reshape(lam,(K,1))\n    G = np.zeros((N*K+K,N*K+N))\n    ffp = (fRets.T @ fRets) / T\n    G[:(N*K),:(N*K)]=kron(eye(N),ffp)\n    G[:(N*K),(N*K):] = kron(eye(N),-lam)\n    G[(N*K):,(N*K):] = -beta.T\n    \n    return G\n\n\n    \n\n\n\n\n\n\n\nNext, the data is imported and a subset of the test portfolios is selected to make the estimation faster.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndata = read_csv('FamaFrench.csv')\n\n# Split using both named colums and ix for larger blocks\ndates = data['date'].values\nfactors = data[['VWMe','SMB','HML']].values\nriskfree = data['RF'].values\nportfolios = data.iloc[:,5:].values\n\nT,N = portfolios.shape\nportfolios = portfolios[:,np.arange(0,N,2)]\nT,N = portfolios.shape\nexcessRet = portfolios - np.reshape(riskfree,(T,1))\nK = np.size(factors,1)\n\n\n    \n\n\n\n\n\n\n\nStarting values for the factor loadings and rick premia are estimated using OLS and simple means.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nbetas = []\nfor i in range(N):\n    res = sm.OLS(excessRet[:,i],sm.add_constant(factors)).fit()\n    betas.append(res.params[1:])\n\navgReturn = excessRet.mean(axis=0)\navgReturn.shape = N,1\nbetas = array(betas)\nres = sm.OLS(avgReturn, betas).fit()\nriskPremia = res.params\n\n\n    \n\n\n\n\n\n\n\nThe starting values are computed the first step estimates are found using the non-linear optimizer.  The initial weighting matrix is just the identify matrix.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nriskPremia.shape = 3\nstartingVals = np.concatenate((betas.flatten(),riskPremia))\n\nWinv = np.eye(N*(K+1))\nargs = (excessRet, factors, Winv)\niteration = 0\nfunctionCount = 0\nstep1opt = fmin_bfgs(gmm_objective, startingVals, args=args, callback=iter_print)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nFunc value: 1915.975414620774, Iteration: 1, Function Count: 132\nFunc value: 1817.0224254364093, Iteration: 2, Function Count: 220\nFunc value: 1814.9526088153193, Iteration: 3, Function Count: 308\nFunc value: 1814.8636328788023, Iteration: 4, Function Count: 396\nFunc value: 1814.7320075212833, Iteration: 5, Function Count: 440\nFunc value: 1814.4944170296885, Iteration: 6, Function Count: 484\nFunc value: 1814.4840096314288, Iteration: 7, Function Count: 572\nFunc value: 1814.4835355894866, Iteration: 8, Function Count: 660\nFunc value: 1814.4834334886873, Iteration: 9, Function Count: 748\nFunc value: 1814.4832402214106, Iteration: 10, Function Count: 792\nFunc value: 1814.483239345376, Iteration: 11, Function Count: 880\nFunc value: 1814.4832044513546, Iteration: 12, Function Count: 1012\nFunc value: 1814.3989963962504, Iteration: 13, Function Count: 1276\nFunc value: 1814.3642859418874, Iteration: 14, Function Count: 1320\nFunc value: 1814.301102018856, Iteration: 15, Function Count: 1364\nFunc value: 1814.301098499327, Iteration: 16, Function Count: 1452\nFunc value: 1814.3010704933515, Iteration: 17, Function Count: 1540\nFunc value: 1814.296612835476, Iteration: 18, Function Count: 1716\nFunc value: 1814.2538448019918, Iteration: 19, Function Count: 1804\nFunc value: 1814.253749266872, Iteration: 20, Function Count: 1892\nFunc value: 1814.2536217543443, Iteration: 21, Function Count: 1936\nFunc value: 1814.2341819587186, Iteration: 22, Function Count: 2112\nFunc value: 1814.2190046927274, Iteration: 23, Function Count: 2156\nFunc value: 1814.1901664290635, Iteration: 24, Function Count: 2200\nFunc value: 1814.1900618989262, Iteration: 25, Function Count: 2288\nFunc value: 1814.1899629209177, Iteration: 26, Function Count: 2332\nFunc value: 1814.1315029565549, Iteration: 27, Function Count: 2552\nFunc value: 1814.1207160483482, Iteration: 28, Function Count: 2640\nFunc value: 1814.120651593227, Iteration: 29, Function Count: 2728\nFunc value: 1814.1206404952559, Iteration: 30, Function Count: 2816\nFunc value: 1814.093987040505, Iteration: 31, Function Count: 3080\nFunc value: 1814.0931557560025, Iteration: 32, Function Count: 3168\nFunc value: 1814.0922310255569, Iteration: 33, Function Count: 3212\nFunc value: 1814.0921898261458, Iteration: 34, Function Count: 3300\nFunc value: 1814.092112795961, Iteration: 35, Function Count: 3344\nFunc value: 1814.080248929288, Iteration: 36, Function Count: 3520\nFunc value: 1814.0799729900195, Iteration: 37, Function Count: 3608\nFunc value: 1814.079961881844, Iteration: 38, Function Count: 3696\nFunc value: 1814.0799614793552, Iteration: 39, Function Count: 3784\nFunc value: 1814.0757650935916, Iteration: 40, Function Count: 4092\nFunc value: 1814.0755830705248, Iteration: 41, Function Count: 4180\nFunc value: 1814.0755746091875, Iteration: 42, Function Count: 4268\nFunc value: 1814.0702842972405, Iteration: 43, Function Count: 4488\nFunc value: 1814.0700243731067, Iteration: 44, Function Count: 4576\nFunc value: 1814.0700110352632, Iteration: 45, Function Count: 4664\nFunc value: 1814.0678482400072, Iteration: 46, Function Count: 4840\nFunc value: 1814.067660339931, Iteration: 47, Function Count: 4928\nFunc value: 1814.067656754271, Iteration: 48, Function Count: 5016\nFunc value: 1814.065377183398, Iteration: 49, Function Count: 5236\nFunc value: 1814.0652521531151, Iteration: 50, Function Count: 5324\nFunc value: 1814.0652503654978, Iteration: 51, Function Count: 5412\nFunc value: 1814.0640861808768, Iteration: 52, Function Count: 5632\nFunc value: 1814.0640022668042, Iteration: 53, Function Count: 5720\nFunc value: 1814.0611488164795, Iteration: 54, Function Count: 5852\nFunc value: 1814.059515832646, Iteration: 55, Function Count: 5896\nFunc value: 1814.0595158325361, Iteration: 56, Function Count: 5940\nFunc value: 1814.0595158325355, Iteration: 57, Function Count: 6072\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 1814.059516\n         Iterations: 57\n         Function evaluations: 9031\n         Gradient evaluations: 205\n\n\n\n\n\n\n\n\n\n\n\nHere we look at the risk premia estimates from the first step (inefficient) estimates.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \npremia = step1opt[-3:]\npremia = Series(premia,index=['VWMe', 'SMB', 'HML'])\nprint('Annualized Risk Premia (First step)')\nprint(12 * premia)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nAnnualized Risk Premia (First step)\nVWMe    5.829995\nSMB     4.068224\nHML     1.680948\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\nNext the first step estimates are used to estimate the moment conditions which are in-turn used to estimate the optimal weighting matrix for the moment conditions.  This is then used as an input for the 2nd-step estimates.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nout = gmm_objective(step1opt, excessRet, factors, Winv, out=True)\nS = np.cov(out[1].T)\nWinv2 = inv(S)\nargs = (excessRet, factors, Winv2)\n\niteration = 0\nfunctionCount = 0\nstep2opt = fmin_bfgs(gmm_objective, step1opt, args=args, callback=iter_print)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nFunc value: 70.69178252370772, Iteration: 1, Function Count: 132\nFunc value: 69.26303959975596, Iteration: 2, Function Count: 176\nFunc value: 67.07244129650894, Iteration: 3, Function Count: 220\nFunc value: 64.57443451479321, Iteration: 4, Function Count: 264\nFunc value: 62.64097306083999, Iteration: 5, Function Count: 308\nFunc value: 60.38315319123633, Iteration: 6, Function Count: 352\nFunc value: 59.77131346063476, Iteration: 7, Function Count: 396\nFunc value: 59.016700262647376, Iteration: 8, Function Count: 440\nFunc value: 58.11824688768306, Iteration: 9, Function Count: 484\nFunc value: 57.16139475771817, Iteration: 10, Function Count: 528\nFunc value: 56.54119670206884, Iteration: 11, Function Count: 572\nFunc value: 55.76261111890216, Iteration: 12, Function Count: 616\nFunc value: 54.70774239263665, Iteration: 13, Function Count: 660\nFunc value: 54.16273697904013, Iteration: 14, Function Count: 748\nFunc value: 53.68442984106602, Iteration: 15, Function Count: 792\nFunc value: 53.24912513313372, Iteration: 16, Function Count: 836\nFunc value: 52.95654923541569, Iteration: 17, Function Count: 880\nFunc value: 52.70763030807515, Iteration: 18, Function Count: 924\nFunc value: 52.40947922763522, Iteration: 19, Function Count: 968\nFunc value: 52.28025850343027, Iteration: 20, Function Count: 1012\nFunc value: 52.0945930956645, Iteration: 21, Function Count: 1056\nFunc value: 51.92591993694722, Iteration: 22, Function Count: 1100\nFunc value: 51.69127887764556, Iteration: 23, Function Count: 1144\nFunc value: 51.32800767550518, Iteration: 24, Function Count: 1188\nFunc value: 50.8832556502003, Iteration: 25, Function Count: 1232\nFunc value: 50.61502081122463, Iteration: 26, Function Count: 1276\nFunc value: 50.3253061036018, Iteration: 27, Function Count: 1320\nFunc value: 49.82326422689157, Iteration: 28, Function Count: 1364\nFunc value: 49.458055584312206, Iteration: 29, Function Count: 1408\nFunc value: 49.30507269503761, Iteration: 30, Function Count: 1452\nFunc value: 49.080604460391186, Iteration: 31, Function Count: 1496\nFunc value: 48.86937171160105, Iteration: 32, Function Count: 1540\nFunc value: 48.76039718096907, Iteration: 33, Function Count: 1628\nFunc value: 48.61522962324979, Iteration: 34, Function Count: 1672\nFunc value: 48.43822338181308, Iteration: 35, Function Count: 1716\nFunc value: 48.223264727455955, Iteration: 36, Function Count: 1760\nFunc value: 48.119297612182464, Iteration: 37, Function Count: 1804\nFunc value: 47.9966953205165, Iteration: 38, Function Count: 1848\nFunc value: 47.82071403838669, Iteration: 39, Function Count: 1892\nFunc value: 47.59984420196816, Iteration: 40, Function Count: 1936\nFunc value: 47.19190580374522, Iteration: 41, Function Count: 1980\nFunc value: 46.46434101774428, Iteration: 42, Function Count: 2024\nFunc value: 46.17952767128317, Iteration: 43, Function Count: 2112\nFunc value: 45.64869841620502, Iteration: 44, Function Count: 2156\nFunc value: 44.79178194363791, Iteration: 45, Function Count: 2200\nFunc value: 44.31246192707072, Iteration: 46, Function Count: 2244\nFunc value: 44.31220746711396, Iteration: 47, Function Count: 2288\nFunc value: 44.31216779746252, Iteration: 48, Function Count: 2332\nFunc value: 44.31216776026349, Iteration: 49, Function Count: 2376\nFunc value: 44.31216775979089, Iteration: 50, Function Count: 2420\nFunc value: 44.31216775977227, Iteration: 51, Function Count: 2464\nFunc value: 44.31216775977222, Iteration: 52, Function Count: 3564\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 44.312168\n         Iterations: 52\n         Function evaluations: 6786\n         Gradient evaluations: 154\n\n\n\n\n\n\n\n\n\n\n\nFinally the VCV of the parameter estimates is computed.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nout = gmm_objective(step2opt, excessRet, factors, Winv2, out=True)\nG = gmm_G(step2opt, excessRet, factors)\nS = np.cov(out[1].T)\nvcv = inv(G @ inv(S) @ G.T)/T\n\n\n    \n\n\n\n\n\n\n\nThe annualized risk premia and their associated t-stats.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \npremia = step2opt[-3:]\npremia = Series(premia,index=['VWMe', 'SMB', 'HML'])\npremia_vcv = vcv[-3:,-3:]\nprint('Annualized Risk Premia')\nprint(12 * premia)\n\npremia_stderr = np.diag(premia_vcv)\npremia_stderr = Series(premia_stderr,index=['VWMe', 'SMB', 'HML'])\nprint('T-stats')\nprint(premia / premia_stderr)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nAnnualized Risk Premia\nVWMe    10.089708\nSMB      3.457167\nHML      7.620110\ndtype: float64\nT-stats\nVWMe    28.282294\nSMB     22.372714\nHML     43.791637\ndtype: float64",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/notebooks/example-gmm-estimation/"
    },
    {
      "title": "Example: LaTeX Output",
      "text": "Example: Exporting to $\\LaTeX$\u00b6The first code block contains the imports needed and defines a flag which determines whether the \noutput $\\LaTeX$ should be compiled.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n# imports\nimport numpy as np\nimport subprocess\n\n# Flag to compile output tables\ncompileLatex = False\n\n\n    \n\n\n\n\n\n\n\nThe next code block loads the npz file created using the output from the Fama-MacBeth example.\nThe second part shows a generic method to restore all variables. The loaded data is in a dictionary,\nand so iterating over the keys and using globals() (a dictionary) in the main program.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \n# Load variables\nf = np.load('fama-macBeth-results.npz')\ndata = f.items()\n# Manually load parameters and std errors\narp = f['arp']\narpSE = f['arpSE']\nbeta = f['beta']\nbetaSE = f['betaSE']\nJ = f['J']\nJpval = f['Jpval']\n\n# Generic restore of all data in a npz file\nfor key in f.keys():\n    globals()[key] = f[key]\nf.close()\n\n\n    \n\n\n\n\n\n\n\nThe document is be stored in a list. The first few lines contain the required header for a\n$\\LaTeX$ document, including some packages used to improve table display and to select a custom font.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n# List to hold table\nlatex = []\n# Initializd LaTeX document\nlatex.append(r'\\documentclass[a4paper]{article}')\nlatex.append(r'\\usepackage{amsmath}')\nlatex.append(r'\\usepackage{booktabs}')\nlatex.append(r'\\usepackage[adobe-utopia]{mathdesign}')\nlatex.append(r'\\usepackage[T1]{fontenc}')\nlatex.append(r'\\begin{document}')\n\n\n    \n\n\n\n\n\n\n\nTable 1 is stored in its own list, and then extend will be used to add it to the main list.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \n# Table 1\ntable1 = []\ntable1.append(r'\\begin{center}')\ntable1.append(r'\\begin{tabular}{lrrr} \\toprule')\n# Header\ncolNames = [r'VWM$e$','SMB','HML']\nheader = ''\nfor cName in colNames:\n    header += ' & ' + cName\n\nheader += r'\\\\ \\cmidrule{2-4}'\ntable1.append(header)\n# Main row\nrow = ''\nfor a,se in zip(arp,arpSE):\n    row += r' & $\\underset{{({0:0.3f})}}{{{1:0.3f}}}$'.format(se,a)\ntable1.append(row)\n# Blank row\nrow = r'\\\\'\ntable1.append(row)\n# J-stat row\nrow = r'J-stat: $\\underset{{({0:0.3f})}}{{{1:0.1f}}}$ \\\\'.format(float(Jpval),float(J))\ntable1.append(row)\ntable1.append(r'\\bottomrule \\end{tabular}')\ntable1.append(r'\\end{center}')\n# Extend latex with table 1\nlatex.extend(table1)\nlatex.append(r'\\newpage')\n\n\n    \n\n\n\n\n\n\n\nTable 2 is a bit more complex, and uses loops to iterate over the rows of the arrays containing\nthe $\\beta$s and their standard errors.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \n# Format information for table 2\nsizes = ['S','2','3','4','B']\nvalues = ['L','2','3','4','H']\n# Table 2 has the same header as table 1, copy with a slice\ntable2 = table1[:3]\nm = 0\nfor i in range(len(sizes)):\n    for j in range(len(values)):\n        row = 'Size: {:}, Value: {:} '.format(sizes[i],values[j])\n        b = beta[:,m]\n        s = betaSE[m,1:]\n        for k in range(len(b)):\n            row += r' & $\\underset{{({0:0.3f})}}{{{1: .3f}}}$'.format(s[k],b[k])\n        row += r'\\\\ '\n        table2.append(row)\n        m += 1\n    if i<(len(sizes)-1):\n        table2.append(r'\\cmidrule{2-4}')\n\ntable2.append(r'\\bottomrule \\end{tabular}')\ntable2.append(r'\\end{center}')\n# Extend with table 2\nlatex.extend(table2)\n\n\n    \n\n\n\n\n\n\n\nThe penultimate block finished the document, and uses write to write the lines to the $\\LaTeX$ file.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n# Finish document   \nlatex.append(r'\\end{document}')\n# Write to table\nfid = open('latex.tex','w')\nfor line in latex:\n    fid.write(line + '\\n')\nfid.close()\n\n\n    \n\n\n\n\n\n\n\nFinally, if the flag is set, subprocess is used to compile the LaTeX.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n# Compile if needed\nif compileLatex:\n    exitStatus = subprocess.run(['pdflatex', 'latex.tex'])",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/notebooks/example-latex-output/"
    },
    {
      "title": "Python Course",
      "text": "This course is an introduction to Python and programming aimed at students working\nin Finance and Economics. The course is designed to be taught using the Jupyter notebooks\nthat are in the course GitHub repository and\nare linked below. The complete course is available for\ndownload as a pdf.\nGitHub\u00b6\nThe introduction is available on Github. \nIf you are happy to use git, you can download everything\nusing git, or for even fork the repo and save your progress to your own fork.\nSelf-paced Study\u00b6\nThis course is intended as an alternative to the MATLAB introduction and so\nis self-paced.  Each of the lessons is covered in one or more videos that are\navailable in a YouTube channel.\nNote: The video content is a work-in-progress and should be completed by mid-October 2019.\nNotebooks\u00b6\n\n\n\n\ufeffLesson\nLink to Download\nDemonstration(s)\n\n\n\n\nInstallation\nDownload\nInstallation\n\n\nLesson 1\nDownload\nLesson 1: Part 1, Part 2, Part 3\n\n\nLesson 2\nDownload\nLesson 2\n\n\nLesson 3\nDownload\nLesson 3\n\n\nLesson 4\nDownload\nLesson 4\n\n\nLesson 5\nDownload\nLesson 5\n\n\nLesson 6\nDownload\nLesson 6\n\n\nLesson 7\nDownload\nLesson 7\n\n\nLesson 8\nDownload\nLesson 8\n\n\nLesson 9\nDownload\nLesson 9\n\n\nLesson 10\nDownload\nLesson 10\n\n\nLesson 11\nDownload\nLesson 11\n\n\nLesson 12\nDownload\nLesson 12\n\n\nLesson 13\nDownload\nLesson 13\n\n\nLesson 14\nDownload\nLesson 14\n\n\nLesson 15\nDownload\nLesson 15\n\n\nLesson 16\nDownload\nLesson 16\n\n\nLesson 17\nDownload\nLesson 17\n\n\nLesson 18\nDownload\nLesson 18\n\n\nLesson 19\nDownload\nLesson 19\n\n\nLesson 20\nDownload\nLesson 20\n\n\nLesson 21\nDownload\nLesson 21\n\n\nLesson 22\nDownload\nLesson 22\n\n\nFinal Exam\nDownload\n\n\n\n\nDownloading: Right-click and select Download to save the notebook files to your computer.\nData and Supporting Files\u00b6\nData",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/python/course/"
    },
    {
      "title": "Specimen",
      "text": "Specimen\u00b6\nBody copy\u00b6\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero,\nmollis sed massa vel, ornare viverra ex. Mauris a ullamcorper lacus. Nullam\nurna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales\npulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan\ntempor.\nSed suscipit, orci non pretium pretium, quam mi gravida metus, vel\nvenenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum\neros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet\nnulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin\ntincidunt. Aenean ullamcorper sit amet nulla at interdum.\nHeadings\u00b6\nThe 3rd level\u00b6\nThe 4th level\u00b6\nThe 5th level\u00b6\nThe 6th level\u00b6\nHeadings with secondary text\u00b6\nThe 3rd level with secondary text\u00b6\nThe 4th level with secondary text\u00b6\nThe 5th level with secondary text\u00b6\nThe 6th level with secondary text\u00b6\nBlockquotes\u00b6\n\nMorbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum.\n  Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc\n  mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad\n  litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie\n  imperdiet consectetur.\n\nBlockquote nesting\u00b6\n\nSed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh, vitae\n  faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem\n  libero fermentum urna, ut efficitur elit ligula et nunc.\n\nMauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla.\n    Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio.\n    Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum.\n    eu odio.\n\nSuspendisse rutrum facilisis risus, eu posuere neque commodo a.\n      Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo\n      bibendum, sodales mauris ut, tincidunt massa.\n\n\n\nOther content blocks\u00b6\n\nVestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\n  lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\n  sit amet laoreet nibh.\n\nvar _extends = function(target) {\nfor (var i = 1; i < arguments.length; i++) {\n  var source = arguments[i];\n  for (var key in source) {\n    target[key] = source[key];\n  }\n}\nreturn target;\n};\n\n\n\n\nPraesent at return target, sodales nibh vel, tempor felis. Fusce\n      vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices.\n      Donec consectetur mauris non neque imperdiet, eget volutpat libero.\n\n\nLists\u00b6\nUnordered lists\u00b6\n\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\n  non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\n  at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.\n\nDuis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\nNam vulputate tincidunt fringilla.\nNullam dignissim ultrices urna non auctor.\n\n\n\nAliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut\n  eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam\n  ac, aliquet sed mauris.\n\n\nNulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur\n  accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh\n  lacinia sed. Aenean in finibus diam.\n\n\nOrdered lists\u00b6\n\n\nInteger vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\n  elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\n  consectetur feugiat sodales.\n\n\nCum sociis natoque penatibus et magnis dis parturient montes, nascetur\n  ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam\n  pellentesque lacinia eu vel odio.\n\n\nVivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet\n  quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a\n  ultricies libero efficitur sed.\n\nMauris dictum mi lacus\nUt sit amet placerat ante\nSuspendisse ac eros arcu\n\n\n\nMorbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet\n  rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed\n  aliquet, neque at rutrum mollis, neque nisi tincidunt nibh.\n\n\nPellentesque eget var _extends ornare tellus, ut gravida mi.\n\n\n\n\nvar _extends = function(target) {\n  for (var i = 1; i < arguments.length; i++) {\n    var source = arguments[i];\n    for (var key in source) {\n      target[key] = source[key];\n    }\n  }\n  return target;\n};\n\n\n\nVivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis\n  sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis\n  nulla. Vivamus a pharetra leo.\n\nDefinition lists\u00b6\n\nLorem ipsum dolor sit amet\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus\ntellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor\nlobortis orci, at elementum urna sodales vitae. In in vehicula nulla.\nDuis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\nNam vulputate tincidunt fringilla.\nNullam dignissim ultrices urna non auctor.\n\nCras arcu libero\n\nAliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin\nut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at\naliquam ac, aliquet sed mauris.\n\n\nCode blocks\u00b6\nInline\u00b6\nMorbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet\nrutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra,\nper inceptos himenaeos. Pellentesque aliquet quam enim, eu volutpat urna\nrutrum a.\nNam vehicula nunc return target mauris, a ultricies libero efficitur\nsed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque\neget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.\nListing\u00b6\n1\n2\n3\n4\n5\n6\n7\n8\n9var _extends = function(target) {\n  for (var i = 1; i < arguments.length; i++) {\n    var source = arguments[i];\n    for (var key in source) {\n      target[key] = source[key];\n    }\n  }\n  return target;\n};\n\n\nHorizontal rules\u00b6\nAenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet\ndui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna\nnon auctor.\n\nInteger vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\nelementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\nconsectetur feugiat sodales.\nData tables\u00b6\n\n\n\nSollicitudo / Pellentesi\nconsectetur\nadipiscing\nelit\narcu\nsed\n\n\n\n\nVivamus a pharetra\nyes\nyes\nyes\nyes\nyes\n\n\nOrnare viverra ex\nyes\nyes\nyes\nyes\nyes\n\n\nMauris a ullamcorper\nyes\nyes\npartial\nyes\nyes\n\n\nNullam urna elit\nyes\nyes\nyes\nyes\nyes\n\n\nMalesuada eget finibus\nyes\nyes\nyes\nyes\nyes\n\n\nUllamcorper\nyes\nyes\nyes\nyes\nyes\n\n\nVestibulum sodales\nyes\n-\nyes\n-\nyes\n\n\nPulvinar nisl\nyes\nyes\nyes\n-\n-\n\n\nPharetra aliquet est\nyes\nyes\nyes\nyes\nyes\n\n\nSed suscipit\nyes\nyes\nyes\nyes\nyes\n\n\nOrci non pretium\nyes\npartial\n-\n-\n-\n\n\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\nnon sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\nat elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.\n\n\n\nLeft\nCenter\nRight\n\n\n\n\nLorem\ndolor\namet\n\n\nipsum\nsit\n\n\n\n\nVestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\nlectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\nsit amet laoreet nibh.\n\n  \n    \n    \n  \n  \n    \n      Table\n      with colgroups (Pandoc)\n    \n  \n  \n    \n      Lorem\n      ipsum dolor sit amet.\n    \n    \n      Sed sagittis\n      eleifend rutrum. Donec vitae suscipit est.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/specimen/"
    },
    {
      "title": "Research",
      "text": "Write your page here.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/research/"
    },
    {
      "title": "LyX",
      "text": "LyX is a powerful WYSIWYG editor\nfor LaTeX. I am an avid user of LyX since it allows for a highly productive environment for \nwriting text and formatted math while continuing to allow for a high degree of customizability. \nLyX is an open source version of Scientific Workplace which is both more modern and has broader\ncompatibility with standard LaTeX.\nLyX is cross platform and works equally well on Windows, Linux or OS X.\nThese videos comprise a basic introduction to LyX and cover the core tools needed to write a \nThesis or paper in Economics or Econometrics (and many other fields, especially in mathematics, \nstatistics or social sciences).\nVideo Introduction\u00b6\n\nInstalling LyX on Windows\nInstalling LyX on Linux\nSetting up a New Document and Basic Structure\nBasic Text Input\nList Environments\nAdding Math\nTables\nFigures\nThe Bibliography\nAdding Custom LaTeX in LyX\nExporting Completed Documents\nPresentations (Beamer) in LyX\n\nWritten Guidance\u00b6\nSome people prefer a written guide. Fortunately (for me), a good guide has been written \n(by others) and is available at Lyx Guide.",
      "tags": "lyx",
      "url": "https://www.kevinsheppard.com/teaching/lyx/"
    },
    {
      "title": "Archived and Other Teaching",
      "text": "Archived Courses\u00b6\nAdvanced Econometrics is full course covering forecasting\nfrom high-dimensional spaces and evaluating forecasts when many models are tried (data snooping).\nOxCort Hacking\u00b6\nOxCORT is painful to use. The console in Firefox or Chrome can be used to automate filling out many\nforms. A simple example is:\nfor (i=0;i<=17;i++)\n{\n    temp = document.getElementById(\"RF_\" + i + \"_toCome\");\n    temp.value = 2;\n    temp = document.getElementById(\"RF_\" + i + \"_workSet\");\n    temp.value = 4;\n    temp = document.getElementById(\"RF_\" + i + \"_workDone\");\n    temp.value = 3;\n    temp = document.getElementById(\"RF_\" + i + \"_workLate\");\n    temp.value = 0;\n    temp = document.getElementById(\"RF_\" + i + \"_industry\");\n    temp.value = \"Very good\";\n    temp = document.getElementById(\"RF_\" + i + \"_progress\");\n    temp.value = \"Very good\";\n    temp = document.getElementById(\"RF_\" + i + \"_grade\");\n    temp.value = \"2.i/1\";;\n}\n\n\nWebsite-related content\u00b6\nSpecimen\nNotebook Specimen",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/other/"
    },
    {
      "title": "MATLAB",
      "text": "Introductory Notes and Course\u00b6\nMATLAB Notes and Course is a set of notes and an introductory\nMATLAB course designed for new users.\nMFE Companion Course\u00b6\nThe companion course is designed to accompany\nFinancial Econometrics I and II and to provide tools needed in Empirical Asset\npricing.  \nMFE Toolbox\u00b6\nThe MFE Toolbox provides a large collection of\nMATLAB functions for analyzing financial time series.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/matlab/"
    },
    {
      "title": "Python",
      "text": "Python Notes\u00b6\nA set of notes that introduce the core concepts of Python that\nare relevant to applications in Statistics, Econometrics and many other\nnumerical areas. Codes Python fundamentals, NumPy, Pandas,\nand some parts of SciPy and statsmodels.  \nPython Notes\nIntroductory Course\u00b6\nA short course designed for people new to Python, and often new\nto programming.  Starts with the basics - getting the Python environment\nright, and work through entering arrays and pandas DataFrame, selecting\nelements, basic looping and graphics.  \nIntroductory Course\nMFE Companion Course\u00b6\nA course that accompanies the MFE Financial Econometrics teaching. It is\ndesigned to complement the lecture and provide the core set fo skills\nneeded to analyze data and fit models. \nCompanion Course",
      "tags": "mfe,python,teaching",
      "url": "https://www.kevinsheppard.com/teaching/python/"
    },
    {
      "title": "Python Notes",
      "text": "Python Notes\u00b6\nIntroduction to Python for Econometrics, Statistics and Numerical Analysis: Fourth+ Edition \nDownload the Notes\nPython is a widely used general purpose programming language, which\nhappens to be well suited to econometrics, data analysis and other more\ngeneral numeric problems. These notes provide an introduction to Python\nfor a beginning programmer. They may also be useful for an experienced\nPython programmer interested in using NumPy, SciPy, matplotlib and\npandas for numerical and statistical analysis (if this is the case, much\nof the beginning can be skipped).\nNew material added to the fifth edition on September 2021.\nChanges\nCurrent Edition\u00b6\nIntroduction to Python for Econometrics, Statistics and Numerical Analysis: Fourth Edition\nArchived Versions\u00b6\nFourth Edition\nThird Edition\nData and Notebooks\u00b6\nData\u00b6\n\nData and Code from the notes. These files are needed to run some of the code in the notes.\nThe Fama-French data set is used in the asset-pricing examples.\nThe FTSE 100 data from 1984 until 2012 is used in the GJR-GARCH example.\n\nNotebooks\u00b6\nThese notebooks contains the four extended examples from the Examples chapter.\n\nEstimation of a GJR-GARCH model (download)\nEstimation of risk premia using Fama-MacBeth (download)\nEstimation using the Generalized Method of Moments (GMM) (download)\nOutputting to LaTeX (download)\n\nMini-course\u00b6\nPython Course",
      "tags": "python",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/"
    },
    {
      "title": "Changes Log",
      "text": "Changes to Introduction to Python for Econometrics, Statistics and Numerical Analysis\u00b6\nFifth Edition\u00b6\n\nPython 3.8 or 3.9 are the recommended versions.\nAdded a chapter on code style.\nExpanded coverage of random number generation and added coverage of the preferred method to generate random variates, numpy.random.Generator..\nVerified that all code and examples work correctly against 2021 versions of modules.\nAdded coverage of context managers (with method as variable) as the preferred way to open and close files.\nChanged examples to use context managers where appropriate.\n\nFourth edition\u00b6\n\nPython 3.8 is the recommended version. The notes require Python 3.6 or later, and all references to Python 2.7 have been removed.\nRemoved references to NumPy's matrix class and clarified that it should not be used.\nVerified that all code and examples work correctly against 2020 versions of modules. The notable packages and their versions are:\nPython 3.8 (Preferred version), 3.6 (Minimum version)\nNumPy: 1.19.1\nSciPy: 1.5.2\npandas: 1.1.1\nmatplotlib: 3.3.1\nIntroduced f-Strings in Section [subsec:f-Strings] as the preferred way to format strings using modern Python. The notes use f-String where possible instead of format.\nAdded coverage of Windowing function \u2013 rolling, expanding and ewm \u2013 to the pandas chapter.\nExpanded the list of packages of interest to researchers working in statistics, econometrics and machine learning.\nExpanded description of model classes and statistical tests in statsmodels that are most relevant for econometrics. Added section detailing formula support. This list represents on a small function of the statsmodels API. \nAdded minimize as the preferred interface for non-linear function optimization in Chapter [chap:Non-linear-Function-Optimization].\nPython 2.7 support has been officially dropped, although most examples continue to work with 2.7. Do not Python 2.7 for numerical code.\nSmall typo fixes, thanks to Marton Huebler.\nFixed direct download of FRED data due to API changes, thanks to Jesper Termansen.\nThanks for Bill Tubbs for a detailed read and multiple typo reports.\nUpdated to changes in line profiler (see Ch. [chap:performance-and-optimization])\nUpdated deprecations in pandas.\nRemoved hold from plotting chapter since this is no longer required.\nThanks for Gen Li for multiple typo reports.\n\nThird edition, Update 1\u00b6\n\nVerified that all code and examples work correctly against 2019 versions of modules. The\n   notable packages and their versions are:\nPython 3.7 (Preferred version)\nNumPy: 1.16\nSciPy: 1.3\npandas: 0.25\nmatplotlib: 3.1\nPython 2.7 support has been officially dropped, although most examples continue to work with 2.7.\n   Do not Python 2.7 in 2019 for numerical code.\n\nThird edition update\u00b6\n\nRewritten installation section focused exclusively on using\n    Continuum\\'s Anaconda.\nPython 3.5 is the default version of Python instead of 2.7. Python\n    3.5 (or newer) is well supported by the Python packages required to\n    analyze data and perform statistical analysis, and bring some new\n    useful features, such as a new operator for matrix multiplication\n    (@).\nRemoved distinction between integers and longs in built-in data\n    types chapter. This distinction is only relevant for Python 2.7.\ndot has been removed from most examples and replaced with @ to\n    produce more readable code.\nSplit Cython and Numba into separate chapters to highlight the\n    improved capabilities of Numba.\nVerified all code working on current versions of core libraries\n    using Python 3.5.\npandas\nUpdated syntax of pandas functions such as resample.\nAdded pandas Categorical.\nExpanded coverage of pandas groupby.\nExpanded coverage of date and time data types and functions.\n\n\nNew chapter introducing statsmodels, a package that facilitates\n    statistical analysis of data. statsmodels includes regression\n    analysis, Generalized Linear Models (GLM) and time-series analysis\n    using ARIMA models.\n\nSecond edition update\u00b6\n\nImproved Cython and Numba sections\nAdded sections discussing interfacing with C code\nAdded sections to the chapter on running code in Parallel covering\n    IPython\\'s cluster server and joblib\nFurther improvements in the installation based on feedback from the\n    Python Course\nUpdated Anaconda to 1.9\nAdded information about using Spyder as an initial IDE.\nAdded packages for Spyder to the installation instructions.\n\nNew in second edition\u00b6\n\nThe preferred installation method is now Continuum Analytics\\'\n    Anaconda. Anaconda is a complete scientific stack and is available\n    for all major platforms.\nNew chapter on pandas. pandas provides a simple but powerful tool to\n    manage data and perform basic analysis. It also greatly simplifies\n    importing and exporting data.\nNew chapter on advanced selection of elements from an array.\nNumba provides just-in-time compilation for numeric Python code\n    which often produces large performance gains when pure NumPy\n    solutions are not available (e.g. looping code).\nAddition to performance section covering line_profiler for\n    profiling code.\nDictionary, set and tuple comprehensions.\nNumerous typos fixed.\nAll code has been verified working against Anaconda 1.7.0.",
      "tags": "python",
      "url": "https://www.kevinsheppard.com/teaching/python/notes/changes/"
    },
    {
      "title": "MFE Teaching Resources",
      "text": "MFE Financial Econometrics Course Sites\u00b6\nThe course websites contain all of the Financial Econometrics I and II material.  \nFinancial Econometrics I (Michaelmas) \nFinancial Econometrics II (Hilary)\nNotes\u00b6\nThe complete set of notes covers all aspects of the Financial Econometrics I and II courses. \nComputation\u00b6\nPython\u00b6\nThe introduction course and companion course\nare designed to accompany Financial Econometrics I and II and\nto provide tools needed in Empirical Asset pricing. This course is substantially self-supported.\nArchive\u00b6\nOlder courses that are not currently offered.\nAdvanced Financial Econometrics: Forecasting (2020)\u00b6\nThe course website for Advanced Financial Econometrics: Forecasting contains all material distributed in Trinity Term 2020.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/teaching/mfe/"
    },
    {
      "title": "Moving to Static Site Generation",
      "text": "This is the first post on my new site.  This site is a static site generated using Nikola, \na static-site generator written in Python. I chose it because I know Python pretty well and\nso can customize any feature I want. It also natively supports Jupyter Notebooks, which is \na big plus. \n\n\nWhile switching over I seriously considered Hugo, a generator\nwritten in Go which seems to have a very large and dedicated base. I may eventually switch \nover. Hugo although the Jupyter integration is nearly non-existent (i.e., manually export to\nmarkdown).\nThe Old Stack\u00b6\nThe good\u00b6\nFor many years I have used mediawiki as a Content Management System. \nI found this to be a relatively simple approach since it allowed be to create and populate a page\nwith links to other documents or files, whether these existed or not. If a page or file did \nnot exist, I could click on its link (which has a 404-color code) and either edit the missing\npage or add the file.  The wiki software also has a number of special pages that let me see\nall missing file.  As a bonus, MediaWiki indexes all content so that the site is immediately \nsearchable. There are many extensions that are helpful in integrating with other services,\ne.g., embedding YouTube videos. \nThe bad\u00b6\nWhat I didn't like about the old site was its fragility.  I regularly received emails telling \nme that my site was down.  These were driven by a connectivity issue with the MariaDB that stored\nthe site's data or with php-fpm which managed the relationship between the nginx front end\nand the PHP engine. My site depended on the correct operation of:\n\nnginx\nMariaDB\nphp-fpm\nMediaWiki\nA DigitalOcean droplet\n\nI also didn't like the feeling that I had to regularly patch the software. While I could setup\nUbuntu to automatically apply patches, I was always nervous about doing this with the MediaWiki\nsoftware. Instead, I would manually patch when I received an email from their security mailing list.\nIn addition, I had to occasionally migrate PHP versions when the current LTS increased its minimum\nsupported version. This was particularly challenging for me since I do not regularly use PHP and so \nI had to reinvent the wheel each time this happened (which I think was only twice, in fairness)\nThe extension landscape was also very uneven.  Many extensions I tried had no long-term support and \nso would continue to work until they used a feature that had been removed. This required repeatedly \nsearching for new extensions, and eventually lead me to remove most of them.  \nThe new site\u00b6\nThe good\u00b6\nThe new site is static in the sense that it is only a collection of HTML, CSS, JavaScript, images\nand other content files.  The only server I need is a webserver (e.g., nginx). While I \nam initially hosting it on the same droplet I used for my MediaWiki installation, I plan \non moving to GitHub pages using Travis CI to build the pages.  This will both save my time and\nmoney.\nThe bad\u00b6\nWhile writing a simple site or pure blog is pretty food proof, Nikola is not a great site generator.\nThe biggest problems I have faced are (some) lack of out-of-box customizability, lack of\na theme I like, and nothing helping me ensure that I do not have missing or orphan pages.\nThe upside for me is that these are all pretty simple to fix writing a few custom plugins. \nFor example, I wrote one that reads the site contents, file a list of files and finds all\nlinks to these files.  It then uses a couple of set operations to determine if there are\nany missing files (really important) or orphan files (less important, but still annoying).",
      "tags": "code,website",
      "url": "https://www.kevinsheppard.com/blog/moving-to-static-site-generation/"
    },
    {
      "title": "index",
      "text": "Background\u00b6\nI currently work at the University of Oxford as a Financial Econometrician. \nMy research focuses on volatility and uncertainty. Measuring and modeling \nconditional correlation, a key input into portfolio risk models is a \ncornerstone of my research.\nI have produced a large volume of teaching resources, including a complete \nset of notes in Financial Econometrics, and introductions to both \nPython and MATLAB. \nI also maintain a number of widely used toolboxes related to my research. The most\nbroadly used of these are the MFE Toolbox for MATLAB, \nand the arch (documentation, \n) \nand linearmodels (documentation, )\nmodules for Python. See my GitHub page for a \ncomplete list of projects.\nI enjoy hiking and being outdoors, although I don't get out as much as I \nwould like. You can see some pictures of my adventures in the gallery. \nI have a sweet but rambunctious chocolate labrador, Callie.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/"
    },
    {
      "title": "Search",
      "text": "Search results appear here.",
      "tags": "",
      "url": "https://www.kevinsheppard.com/search/"
    }
  ]
};